{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustavotarginovalente/Redes-Neurais-Artificiais-para-Simpatizantes/blob/main/Oficina_de_Redes_Neurais_12_11_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. História das Redes Neurais - Origens (1940-1950)\n",
        "\n",
        "O que é: Os primeiros modelos matemáticos inspirados no cérebro humano.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    1943: Warren McCulloch e Walter Pitts criaram o primeiro modelo matemático de neurônio, baseado no funcionamento dos neurônios biológicos\n",
        "\n",
        "    Características: Modelo binário simples que usava operações lógicas (AND, OR, NOT)\n",
        "\n",
        "    Limitação: Não podia aprender - os pesos eram fixos\n",
        "\n",
        "    Importância: Estabeleceu a base teórica provando que redes neurais poderiam, em teoria, computar qualquer função"
      ],
      "metadata": {
        "id": "G-yy4sjso11t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspiração biológica - o neurônio artificial\n",
        "class NeuronioMcCullochPitts:\n",
        "    \"\"\"\n",
        "    Modelo pioneiro (1943) baseado no neurônio biológico\n",
        "    McCulloch e Pitts criaram o primeiro modelo matemático de neurônio\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Lista para armazenar as entradas do neurônio\n",
        "        self.entradas = []\n",
        "        # Lista para armazenar os pesos das conexões\n",
        "        self.pesos = []\n",
        "        # Valor de limiar para ativação\n",
        "        self.limiar = 0\n",
        "\n",
        "    def ativar(self, entradas, pesos, limiar):\n",
        "        # Calcula a soma ponderada das entradas\n",
        "        soma = sum(e * p for e, p in zip(entradas, pesos))\n",
        "        # A função de ativação: retorna 1 se a soma for maior ou igual ao limiar, 0 caso contrário\n",
        "        return 1 if soma >= limiar else 0\n",
        "\n",
        "\n",
        "# DEMONSTRAÇÃO PRÁTICA DO NEURÔNIO McCULLOCH-PITTS PARA FUNÇÃO \"AND\" E \"OR\"\n",
        "\n",
        "print(\"=== DEMONSTRAÇÃO DO NEURÔNIO McCULLOCH-PITTS (1943) ===\")\n",
        "print(\"Modelo matemático pioneiro baseado no neurônio biológico\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Criar instância do neurônio\n",
        "neuronio = NeuronioMcCullochPitts()\n",
        "\n",
        "# Todas as combinações possíveis\n",
        "combinacoes = [\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ]\n",
        "\n",
        "\n",
        "# Configuração para aplicação da função AND (E)\n",
        "\n",
        "print(\"\\n1. SIMULANDO O NEURÔNIO PARA A FUNÇÃO AND (E):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "pesos_and = [1, 1]      # Pesos iguais para ambas entradas\n",
        "limiar_and = 2          # Limiar = 2\n",
        "\n",
        "for entradas in combinacoes:\n",
        "  saida = neuronio.ativar(entradas, pesos_and, limiar_and)\n",
        "  print(f\" Entradas={entradas}, Saída={saida}\")\n",
        "\n",
        "# Configuração para aplicação da função OR (OU)\n",
        "print(\"\\n2. SIMULANDO O NEURÔNIO PARA A FUNÇÃO OR (OU):\")\n",
        "print(\"-\" * 40)\n",
        "pesos_or = [1, 1]      # Pesos iguais para ambas entradas\n",
        "limiar_or = 1          # Limiar = 1\n",
        "\n",
        "for entradas in combinacoes:\n",
        "  saida = neuronio.ativar(entradas, pesos_or, limiar_or)\n",
        "  print(f\" Entradas={entradas}, Saída={saida}\")\n",
        "\n",
        "print(\"\\n3. CARACTERÍSTICAS DO MODELO McCULLOCH-PITTS:\")\n",
        "print(\"-\" *50)\n",
        "print(\" Primeiro modelo matemático de neurônio artificial (1943)\")\n",
        "print(\" Inspirado no funcionamento do cérebro biológico\")\n",
        "print(\" Operação binária: ativa (1) ou não ativa (0)\")\n",
        "print(\" Usa soma ponderada e função degrau. Obs.: Não tem o Bias\")\n",
        "print(\" Base para modelos mais complexos como o Perceptron\")"
      ],
      "metadata": {
        "id": "ZQcGFgaytwX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perceptron (1958)\n",
        "\n",
        "O que é: É um neurônio artificial capaz de realizar aprendizado para redes neurais.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Criado por: Frank Rosenblatt\n",
        "\n",
        "    Funcionamento: Modelo de uma única camada que podia aprender pesos automaticamente\n",
        "\n",
        "    Aplicação: Classificação de padrões linearmente separáveis\n",
        "\n",
        "    Limitação: Minsky e Papert provaram em 1969 que não podia resolver problemas não-lineares como XOR\n",
        "\n",
        "    Legado: Introduziu o conceito de aprendizado por ajuste de pesos"
      ],
      "metadata": {
        "id": "1CBDRgzeqRFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moldeo Perceptron - Sem aprendizado de máquina\n",
        "class ModeloPerceptron:\n",
        "    \"\"\"\n",
        "    Perceptron simples para classificação binária - Sem aprendizado de máquina\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Lista para armazenar as entradas do neurônio\n",
        "        self.entradas = []\n",
        "        # Lista para armazenar os pesos das conexões\n",
        "        self.pesos = []\n",
        "        # Valor de limiar para ativação\n",
        "        self.limiar = 0\n",
        "        #Valor do Bias\n",
        "        self.bias = 0\n",
        "\n",
        "    def ativar(self, entradas, pesos, limiar, bias):\n",
        "        # Calcula a soma ponderada das entradas\n",
        "        soma = sum(e * p for e, p in zip(entradas, pesos)) + bias\n",
        "        # A função de ativação: retorna 1 se a soma for maior ou igual ao limiar, 0 caso contrário\n",
        "        return 1 if soma >= limiar else 0\n",
        "\n",
        "# DEMONSTRAÇÃO PRÁTICA DO PERCEPTRON PARA FUNÇÃO \"AND\" E \"OR\"\n",
        "\n",
        "print(\"=== DEMONSTRAÇÃO DO NEURÔNIO PERCEPTRON ===\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# Criar instância do neurônio\n",
        "neuronio = ModeloPerceptron()\n",
        "\n",
        "# Todas as combinações possíveis\n",
        "combinacoes = [\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ]\n",
        "\n",
        "\n",
        "# Configuração para aplicação da função AND (E)\n",
        "\n",
        "print(\"\\n1. SIMULANDO O NEURÔNIO PERCETRON PARA A FUNÇÃO AND (E):\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "pesos_and = [1, 1]      # Pesos iguais para ambas entradas\n",
        "limiar_and = 0          # Limiar = 0\n",
        "bias_and = -2           # Bias\n",
        "\n",
        "for entradas in combinacoes:\n",
        "  saida = neuronio.ativar(entradas, pesos_and, limiar_and, bias_and)\n",
        "  print(f\" Entradas={entradas}, Saída={saida}\")\n",
        "\n",
        "print(\"\\n2. SIMULANDO O NEURÔNIO PERCETRON PARA A FUNÇÃO OR (OU):\")\n",
        "print(\"-\" * 40)\n",
        "print(\"E o resultado?\")\n",
        "print(\"Esse é com você, fica como dever de casa!! :)\")\n"
      ],
      "metadata": {
        "id": "12LyfmxnuakZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Moldeo Perceptron - Com aprendizado de máquina\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    \"\"\"Perceptron simples para classificação binária - Com aprendizado de máquina\"\"\"\n",
        "\n",
        "    def __init__(self, taxa_aprendizado, n_entradas=2):\n",
        "        # Inicializa pesos e viés com zeros\n",
        "        self.pesos = np.zeros(n_entradas)      # Pesos das conexões\n",
        "        self.viés = 0.0                        # Viés (bias)\n",
        "        self.taxa_aprendizado = taxa_aprendizado  # Taxa de aprendizado\n",
        "\n",
        "    def prever(self, entradas):\n",
        "        \"\"\"Faz previsão para uma entrada\"\"\"\n",
        "        # Calcula soma ponderada: w1*x1 + w2*x2 + ... + wn*xn + viés\n",
        "        soma = np.dot(entradas, self.pesos) + self.viés\n",
        "        # Função de ativação: retorna 1 se soma >= 0, senão 0\n",
        "        return 1 if soma >= 0 else 0\n",
        "\n",
        "    def treinar(self, X, y, epocas=100):\n",
        "        \"\"\"Treina o perceptron\"\"\"\n",
        "        print(\"Iniciando treinamento do Perceptron...\")\n",
        "\n",
        "        for epoca in range(epocas):\n",
        "            erro_total = 0\n",
        "            print(f\"\\nÉpoca {epoca}:\")\n",
        "\n",
        "            for i in range(len(X)):\n",
        "                # Faz previsão para exemplo atual\n",
        "                previsao = self.prever(X[i])\n",
        "                # Calcula erro\n",
        "                erro = y[i] - previsao\n",
        "                erro_total += abs(erro)\n",
        "\n",
        "                print(f\"Entrada: {X[i]} | Pesos: {self.pesos} | Viés: {self.viés} | Erro: {erro_total} | Previsão: {previsao} | Esperado: {y[i]}\")\n",
        "\n",
        "                # Atualiza pesos e viés\n",
        "                self.pesos += self.taxa_aprendizado * erro * X[i]\n",
        "                self.viés += self.taxa_aprendizado * erro\n",
        "\n",
        "            # Print de progresso\n",
        "            if epoca % 10 == 0:\n",
        "                print(f\"Época {epoca}: Erro total = {erro_total}\")\n",
        "\n",
        "            # Para se convergiu\n",
        "            if erro_total == 0:\n",
        "                print(\"-\" *100)\n",
        "\n",
        "                break\n",
        "\n",
        "        print(\"Treinamento concluído, uhu!\")\n",
        "        print(f\"Convergiu na época {epoca}!\")\n",
        "        print(f\"Pesos finais: {self.pesos}\")\n",
        "        print(f\"Viés final: {self.viés}\")\n",
        "\n",
        "# Exemplo de uso\n",
        "print(\"=== EXEMPLO PERCEPTRON - PORTA LÓGICA AND ===\")\n",
        "\n",
        "# Dados de treino: porta AND\n",
        "X = np.array([\n",
        "    [0, 0],  # Entrada 1\n",
        "    [0, 1],  # Entrada 2\n",
        "    [1, 0],  # Entrada 3\n",
        "    [1, 1]   # Entrada 4\n",
        "])\n",
        "y = np.array([0, 0, 0, 1])  # Saídas esperadas\n",
        "\n",
        "# Cria e treina perceptron\n",
        "perceptron = Perceptron(taxa_aprendizado=0.01)\n",
        "perceptron.treinar(X, y, epocas=50)\n",
        "\n",
        "# Testa o modelo\n",
        "print(\"\\n=== TESTE DO MODELO ===\")\n",
        "for i in range(len(X)):\n",
        "    previsao = perceptron.prever(X[i])\n",
        "    print(f\"Entrada: {X[i]} → Previsão: {previsao} (Esperado: {y[i]})\")\n",
        "\n",
        "print(\"-\" *100)\n",
        "print(\"\\n1. CARACTERÍSTICAS DO MODELO:\")\n",
        "print(\"Introduzio o conceito de ajuste dos pesos possibilitando o aprendizado de máquina\")\n",
        "print(\"Porém, é limitado para resolver problemas não-lineares como XOR\")\n"
      ],
      "metadata": {
        "id": "fVpYblyCu5hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverno das Redes Neurais (1969-1980)\n",
        " - Este período não teve código significativo devido às limitações identificadas\n",
        " - Marvin Minsky provou que perceptrons simples não podiam resolver problemas\n",
        " não linearmente separáveis como o XOR"
      ],
      "metadata": {
        "id": "LoV0PHQct0nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Renascimento (1980-1990)\n",
        "    Backpropagation: Algoritmo para treinar redes multicamadas eficientemente\n",
        "\n",
        "    MLP (Multi-Layer Perceptron): Redes com camadas ocultas que podiam resolver problemas não-lineares\n",
        "\n",
        "    Avanços:\n",
        "\n",
        "        Solução do problema XOR\n",
        "\n",
        "        Aplicações práticas em reconhecimento de padrões\n",
        "\n",
        "        Desenvolvimento de arquiteturas mais complexas"
      ],
      "metadata": {
        "id": "bHY1Qe6OuHYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "import time\n",
        "\n",
        "class MLPBackpropagation:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron com Backpropagation - Implementação Histórica\n",
        "    Este algoritmo resolveu o problema XOR e revitalizou as redes neurais nos anos 80\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, arquitetura=[2, 2, 1], taxa_aprendizado=0.5):\n",
        "        \"\"\"\n",
        "        Inicializa a MLP com backpropagation\n",
        "\n",
        "        Args:\n",
        "            arquitetura (list): Número de neurônios em cada camada [entrada, oculta, saída]\n",
        "            taxa_aprendizado (float): Taxa de aprendizado para atualização dos pesos\n",
        "        \"\"\"\n",
        "        print(\" Inicializando MLP com Backpropagation...\")\n",
        "        print(f\"   Arquitetura: {arquitetura}\")\n",
        "        print(f\"   Taxa de aprendizado: {taxa_aprendizado}\")\n",
        "\n",
        "        self.arquitetura = arquitetura\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "\n",
        "        # Inicializa pesos e vieses aleatoriamente\n",
        "        self.pesos = []\n",
        "        self.vieses = []\n",
        "        self.inicializar_pesos()\n",
        "\n",
        "        # Para acompanhamento do treinamento\n",
        "        self.erro_historico = []\n",
        "        self.epoca_historico = []\n",
        "\n",
        "    def inicializar_pesos(self):\n",
        "        \"\"\"Inicializa pesos e vieses com valores aleatórios pequenos\"\"\"\n",
        "        print(\"  Inicializando pesos e vieses...\")\n",
        "\n",
        "        for i in range(len(self.arquitetura) - 1):\n",
        "            # Pesos: conectam camada i para camada i+1\n",
        "            # Valores entre -1 e 1 para facilitar convergência\n",
        "            peso_camada = np.random.uniform(\n",
        "                -1, 1,\n",
        "                (self.arquitetura[i], self.arquitetura[i + 1])\n",
        "            )\n",
        "            vies_camada = np.random.uniform(-1, 1, (1, self.arquitetura[i + 1]))\n",
        "\n",
        "            self.pesos.append(peso_camada)\n",
        "            self.vieses.append(vies_camada)\n",
        "\n",
        "            print(f\"   Camada {i}→{i+1}: {self.arquitetura[i]} → {self.arquitetura[i+1]} \"\n",
        "                  f\"({peso_camada.shape[0]}x{peso_camada.shape[1]} pesos)\")\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Função de ativação sigmoid: 1 / (1 + e^(-x))\n",
        "\n",
        "        Características:\n",
        "        - Saída entre 0 e 1 (boa para probabilidades)\n",
        "        - Suave e diferenciável (necessário para backpropagation)\n",
        "        - Introduz não-linearidade na rede\n",
        "        \"\"\"\n",
        "        # Clip para evitar overflow em exponenciais grandes\n",
        "        x = np.clip(x, -250, 250)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def derivada_sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Derivada da função sigmoid: sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "        No backpropagation, usamos a derivada para:\n",
        "        1. Calcular o gradiente do erro\n",
        "        2. Determinar a direção de atualização dos pesos\n",
        "        3. Ajustar a magnitude do passo de aprendizado\n",
        "        \"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Propagação forward: calcula saída da rede para uma entrada X\n",
        "\n",
        "        Processo:\n",
        "        1. Entrada → Camada oculta: soma ponderada + ativação\n",
        "        2. Camada oculta → Saída: soma ponderada + ativação\n",
        "        3. Retorna saída final e armazena ativações para backpropagation\n",
        "        \"\"\"\n",
        "        self.ativacoes = [X]  # Camada de entrada\n",
        "        self.somas_ponderadas = []  # Valores antes da ativação (z)\n",
        "\n",
        "        # Propaga através de cada camada\n",
        "        for i in range(len(self.pesos)):\n",
        "            # Calcula soma ponderada: z = W·A + b\n",
        "            z = np.dot(self.ativacoes[-1], self.pesos[i]) + self.vieses[i]\n",
        "            self.somas_ponderadas.append(z)\n",
        "\n",
        "            # Aplica função de ativação\n",
        "            a = self.sigmoid(z)\n",
        "            self.ativacoes.append(a)\n",
        "\n",
        "        return self.ativacoes[-1]\n",
        "\n",
        "    def backward(self, X, y, saida):\n",
        "        \"\"\"\n",
        "        Backpropagation: calcula gradientes e atualiza pesos\n",
        "\n",
        "        Este é o algoritmo revolucionário que:\n",
        "        - Propaga o erro de volta através da rede\n",
        "        - Calcula como cada peso contribui para o erro total\n",
        "        - Atualiza pesos para minimizar o erro\n",
        "\n",
        "        Processo:\n",
        "        1. Calcula erro na camada de saída\n",
        "        2. Propaga erro para camadas anteriores\n",
        "        3. Calcula gradientes para cada peso\n",
        "        4. Atualiza pesos usando gradiente descendente\n",
        "        \"\"\"\n",
        "        # Listas para armazenar gradientes\n",
        "        gradientes_pesos = [np.zeros_like(w) for w in self.pesos]\n",
        "        gradientes_vieses = [np.zeros_like(b) for b in self.vieses]\n",
        "\n",
        "        # === PASSO 1: Calcula erro na camada de saída ===\n",
        "        # Erro = diferença entre saída esperada e saída calculada\n",
        "        erro = saida - y\n",
        "\n",
        "        # Delta da camada de saída = erro × derivada da ativação\n",
        "        delta = erro * self.derivada_sigmoid(saida)\n",
        "\n",
        "        # === PASSO 2: Backpropagation através das camadas ===\n",
        "        # Começa da última camada e vai até a primeira\n",
        "        for i in range(len(self.pesos) - 1, -1, -1):\n",
        "            # Gradiente dos pesos = ativação_anterior · delta_atual\n",
        "            gradientes_pesos[i] = np.dot(self.ativacoes[i].T, delta)\n",
        "\n",
        "            # Gradiente dos vieses = delta_atual (soma sobre batch)\n",
        "            gradientes_vieses[i] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            # Se não é a primeira camada, propaga o erro para trás\n",
        "            if i > 0:\n",
        "                # Delta da camada anterior = delta_atual · pesos^T × derivada_ativacao\n",
        "                delta = np.dot(delta, self.pesos[i].T) * self.derivada_sigmoid(self.ativacoes[i])\n",
        "\n",
        "        # === PASSO 3: Atualiza pesos e vieses ===\n",
        "        for i in range(len(self.pesos)):\n",
        "            self.pesos[i] -= self.taxa_aprendizado * gradientes_pesos[i]\n",
        "            self.vieses[i] -= self.taxa_aprendizado * gradientes_vieses[i]\n",
        "\n",
        "        return np.mean(erro ** 2)  # Retorna erro quadrático médio\n",
        "\n",
        "    def treinar(self, X, y, epocas=10000, tolerancia=1e-6, verbose=True):\n",
        "        \"\"\"\n",
        "        Treina a rede neural usando backpropagation\n",
        "\n",
        "        Args:\n",
        "            X (array): Dados de entrada\n",
        "            y (array): Saídas esperadas\n",
        "            epocas (int): Número máximo de épocas\n",
        "            tolerancia (float): Tolerância para convergência\n",
        "            verbose (bool): Se deve imprimir progresso\n",
        "        \"\"\"\n",
        "        print(f\" Iniciando treinamento por {epocas} épocas...\")\n",
        "        print(f\"   Tolerância: {tolerancia}\")\n",
        "\n",
        "        inicio_tempo = time.time()\n",
        "\n",
        "        for epoca in range(epocas):\n",
        "            # Forward pass: calcula saída\n",
        "            saida = self.forward(X)\n",
        "\n",
        "            # Backward pass: atualiza pesos e calcula erro\n",
        "            erro = self.backward(X, y, saida)\n",
        "\n",
        "            # Armazena histórico\n",
        "            self.erro_historico.append(erro)\n",
        "            self.epoca_historico.append(epoca)\n",
        "\n",
        "            # Verifica convergência\n",
        "            if erro < tolerancia:\n",
        "                if verbose:\n",
        "                    print(f\" Convergiu na época {epoca}! Erro: {erro:.8f}\")\n",
        "                break\n",
        "\n",
        "            # Print de progresso\n",
        "            if verbose and epoca % 1000 == 0:\n",
        "                print(f\"  Época {epoca:5d} | Erro: {erro:.8f}\")\n",
        "\n",
        "        tempo_total = time.time() - inicio_tempo\n",
        "        print(f\" Treinamento concluído em {tempo_total:.2f} segundos\")\n",
        "        print(f\"   Épocas: {epoca + 1}, Erro final: {erro:.8f}\")\n",
        "\n",
        "    def prever(self, X):\n",
        "        \"\"\"Faz previsões para novos dados\"\"\"\n",
        "        return self.forward(X)\n",
        "\n",
        "    def avaliar(self, X, y):\n",
        "        \"\"\"Avalia a performance do modelo\"\"\"\n",
        "        previsoes = self.prever(X)\n",
        "        erro = np.mean((previsoes - y) ** 2)\n",
        "        acuracia = np.mean((previsoes > 0.5) == (y > 0.5))\n",
        "\n",
        "        print(f\" Avaliação do modelo:\")\n",
        "        print(f\"   Erro MSE: {erro:.6f}\")\n",
        "        print(f\"   Acurácia: {acuracia:.4f}\")\n",
        "\n",
        "        return {'erro': erro, 'acuracia': acuracia}\n",
        "\n",
        "    def visualizar_treinamento(self):\n",
        "        \"\"\"Visualiza a curva de aprendizado\"\"\"\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.epoca_historico, self.erro_historico, 'b-', linewidth=2)\n",
        "        plt.title('Curva de Aprendizado - Backpropagation')\n",
        "        plt.xlabel('Época')\n",
        "        plt.ylabel('Erro (MSE)')\n",
        "        plt.yscale('log')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # Últimas 100 épocas para ver detalhes da convergência\n",
        "        if len(self.erro_historico) > 100:\n",
        "            ultimas_100 = self.erro_historico[-100:]\n",
        "            epocas_100 = self.epoca_historico[-100:]\n",
        "            plt.plot(epocas_100, ultimas_100, 'r-', linewidth=2)\n",
        "            plt.title('Convergência Final (últimas 100 épocas)')\n",
        "            plt.xlabel('Época')\n",
        "            plt.ylabel('Erro (MSE)')\n",
        "            plt.yscale('log')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 1: PROBLEMA XOR - O PROBLEMA QUE REVOLUCIONOU AS REDES NEURAIS\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 1: PROBLEMA XOR - MARCO HISTÓRICO\")\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        "PROBLEMA XOR (EXCLUSIVE OR):\n",
        "- O perceptron simples NÃO consegue resolver XOR\n",
        "- Minsky & Papert provaram esta limitação em 1969\n",
        "- Isto causou o 'inverno das redes neurais'\n",
        "- Backpropagation em MLPs RESOLVEU este problema!\n",
        "\"\"\")\n",
        "\n",
        "# Dados do problema XOR\n",
        "X_xor = np.array([\n",
        "    [0, 0],  # Entrada 1\n",
        "    [0, 1],  # Entrada 2\n",
        "    [1, 0],  # Entrada 3\n",
        "    [1, 1]   # Entrada 4\n",
        "])\n",
        "y_xor = np.array([[0], [1], [1], [0]])  # XOR: 1 quando entradas são diferentes\n",
        "\n",
        "print(\" Dados de treino (XOR):\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"   {X_xor[i]} → {y_xor[i][0]}\")\n",
        "\n",
        "# Cria MLP com arquitetura mínima para resolver XOR\n",
        "# [2, 2, 1]: 2 entradas, 2 neurônios ocultos, 1 saída\n",
        "mlp_xor = MLPBackpropagation(arquitetura=[2, 2, 1], taxa_aprendizado=0.5)\n",
        "\n",
        "print(\"\\n EXPLICAÇÃO DA ARQUITETURA [2, 2, 1]:\")\n",
        "print(\"   - 2 entradas: x1, x2\")\n",
        "print(\"   - 2 neurônios ocultos: aprendem features não-lineares\")\n",
        "print(\"   - 1 saída: resultado do XOR\")\n",
        "print(\"   - A camada oculta permite combinações não-lineares!\")\n",
        "\n",
        "# Treina a rede\n",
        "mlp_xor.treinar(X_xor, y_xor, epocas=10000, tolerancia=1e-4)\n",
        "\n",
        "# Testa o modelo\n",
        "print(\"\\n TESTE DO MODELO XOR:\")\n",
        "for i in range(len(X_xor)):\n",
        "    previsao = mlp_xor.prever(X_xor[i:i+1])[0][0]\n",
        "    esperado = y_xor[i][0]\n",
        "    status =  \"Correto\" if abs(previsao - esperado) < 0.1 else \"Incorreto\"\n",
        "    print(f\"   {status} Entrada: {X_xor[i]} → Previsto: {previsao:.4f} (Esperado: {esperado})\")\n",
        "\n",
        "# Visualiza aprendizado\n",
        "mlp_xor.visualizar_treinamento()\n",
        "\n",
        "\n",
        "# EXEMPLO 2: PROBLEMA DE CLASSIFICAÇÃO NÃO LINEAR\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 2: CLASSIFICAÇÃO NÃO LINEAR COMPLEXA\")\n",
        "\n",
        "\n",
        "# Cria dataset não linearmente separável\n",
        "X_nao_linear, y_nao_linear = make_classification(\n",
        "    n_samples=200,\n",
        "    n_features=2,\n",
        "    n_redundant=0,\n",
        "    n_informative=2,\n",
        "    n_clusters_per_class=1,\n",
        "    flip_y=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "y_nao_linear = y_nao_linear.reshape(-1, 1)\n",
        "\n",
        "print(f\" Dataset não linear: {X_nao_linear.shape}\")\n",
        "print(f\" Classes: {np.unique(y_nao_linear)}\")\n",
        "\n",
        "# Cria MLP com arquitetura maior\n",
        "mlp_nao_linear = MLPBackpropagation(\n",
        "    arquitetura=[2, 8, 4, 1],  # Mais camadas para problema complexo\n",
        "    taxa_aprendizado=0.1\n",
        ")\n",
        "\n",
        "# Treina\n",
        "mlp_nao_linear.treinar(X_nao_linear, y_nao_linear, epocas=5000)\n",
        "\n",
        "# Avalia\n",
        "resultados = mlp_nao_linear.avaliar(X_nao_linear, y_nao_linear)\n",
        "\n",
        "# Visualização da decisão\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Dados originais\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_nao_linear[:, 0], X_nao_linear[:, 1], c=y_nao_linear.flatten(),\n",
        "           cmap='viridis', alpha=0.7)\n",
        "plt.title('Dados Originais - Não Linearmente Separáveis')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Classe')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Superfície de decisão\n",
        "plt.subplot(1, 3, 2)\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(X_nao_linear[:, 0].min()-0.5, X_nao_linear[:, 0].max()+0.5, 100),\n",
        "    np.linspace(X_nao_linear[:, 1].min()-0.5, X_nao_linear[:, 1].max()+0.5, 100)\n",
        ")\n",
        "Z = mlp_nao_linear.prever(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X_nao_linear[:, 0], X_nao_linear[:, 1], c=y_nao_linear.flatten(),\n",
        "           cmap='viridis', alpha=0.7)\n",
        "plt.title('Superfície de Decisão da MLP')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.colorbar(label='Probabilidade Classe 1')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Curva de aprendizado\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(mlp_nao_linear.epoca_historico, mlp_nao_linear.erro_historico, 'r-', linewidth=2)\n",
        "plt.title('Curva de Aprendizado')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Erro (MSE)')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 3: COMPARAÇÃO COM PERCEPTRON SIMPLES\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 3: COMPARAÇÃO MLP vs PERCEPTRON SIMPLES\")\n",
        "\n",
        "\n",
        "class PerceptronSimples:\n",
        "    \"\"\"Perceptron simples (sem camadas ocultas) para comparação\"\"\"\n",
        "\n",
        "    def __init__(self, taxa_aprendizado=0.1):\n",
        "        self.taxa_aprendizado = taxa_aprendizado\n",
        "        self.pesos = None\n",
        "        self.erro_historico = []\n",
        "\n",
        "    def treinar(self, X, y, epocas=1000):\n",
        "        n_amostras, n_features = X.shape\n",
        "        self.pesos = np.zeros(n_features)\n",
        "        self.vies = 0\n",
        "\n",
        "        for epoca in range(epocas):\n",
        "            erro_total = 0\n",
        "            for i in range(n_amostras):\n",
        "                # Forward pass\n",
        "                saida = self.prever(X[i])\n",
        "                erro = y[i] - saida\n",
        "                erro_total += abs(erro)\n",
        "\n",
        "                # Atualização de pesos (regra do perceptron)\n",
        "                self.pesos += self.taxa_aprendizado * erro * X[i]\n",
        "                self.vies += self.taxa_aprendizado * erro\n",
        "\n",
        "            self.erro_historico.append(erro_total)\n",
        "\n",
        "            if erro_total == 0:\n",
        "                break\n",
        "\n",
        "    def prever(self, X):\n",
        "        # Função degrau\n",
        "        return 1 if np.dot(X, self.pesos) + self.vies >= 0 else 0\n",
        "\n",
        "# Testa ambos no problema XOR\n",
        "print(\" PERCEPTRON SIMPLES vs MLP NO PROBLEMA XOR:\")\n",
        "\n",
        "# Perceptron simples\n",
        "perceptron = PerceptronSimples(taxa_aprendizado=0.1)\n",
        "perceptron.treinar(X_xor, y_xor.flatten(), epocas=1000)\n",
        "\n",
        "print(\"\\n RESULTADOS PERCEPTRON SIMPLES:\")\n",
        "for i in range(len(X_xor)):\n",
        "    previsao = perceptron.prever(X_xor[i])\n",
        "    esperado = y_xor[i][0]\n",
        "    status =  \"Correto\" if previsao == esperado else \"Incorreto\"\n",
        "    print(f\"   {status} Entrada: {X_xor[i]} → Previsto: {previsao} (Esperado: {esperado})\")\n",
        "\n",
        "print(\"\\n RESULTADOS MLP COM BACKPROPAGATION:\")\n",
        "for i in range(len(X_xor)):\n",
        "    previsao = mlp_xor.prever(X_xor[i:i+1])[0][0]\n",
        "    esperado = y_xor[i][0]\n",
        "    status =  \"Correto\" if abs(previsao - esperado) < 0.1 else \"Incorreto\"\n",
        "    print(f\"   {status} Entrada: {X_xor[i]} → Previsto: {previsao:.4f} (Esperado: {esperado})\")\n",
        "\n",
        "print(f\"\\n CONCLUSÃO:\")\n",
        "print(f\"   • Perceptron simples: NÃO resolve XOR (limitação teórica)\")\n",
        "print(f\"   • MLP com backpropagation: RESOLVE XOR!\")\n",
        "print(f\"   • Backpropagation permitiu redes com múltiplas camadas\")\n",
        "print(f\"   • Isto revitalizou a pesquisa em redes neurais nos anos 80\")\n",
        "\n",
        "# =============================================================================\n",
        "# EXEMPLO 4: ANÁLISE DETALHADA DO BACKPROPAGATION\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 4: ENTENDENDO O BACKPROPAGATION PASSO A PASSO\")\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        " COMO O BACKPROPAGATION FUNCIONA:\n",
        "\n",
        "1. FORWARD PASS:\n",
        "   • Dados fluem da entrada para a saída\n",
        "   • Cada neurônio calcula: z = ∑(w·x) + b\n",
        "   • Aplica função de ativação: a = σ(z)\n",
        "\n",
        "2. CALCULA ERRO:\n",
        "   • Compara saída esperada com saída calculada\n",
        "   • Erro = saida_esperada - saida_calculada\n",
        "\n",
        "3. BACKWARD PASS (REVOLUCIONÁRIO):\n",
        "   • Propaga o erro de VOLTA pela rede\n",
        "   • Calcula como cada peso contribuiu para o erro\n",
        "   • Usa a REGRA DA CADEIA do cálculo diferencial\n",
        "\n",
        "4. ATUALIZA PESOS:\n",
        "   • Ajusta pesos na direção que reduz o erro\n",
        "   • novo_peso = peso_antigo - η × ∂erro/∂peso\n",
        "\n",
        " POR QUE FOI TÃO IMPORTANTE:\n",
        "• Resolveu o problema do vanishing gradient (parcialmente)\n",
        "• Permitiu treinar redes com múltiplas camadas\n",
        "• Provou que redes neurais podem aprender funções complexas\n",
        "• Base para todo o Deep Learning moderno!\n",
        "\"\"\")\n",
        "\n",
        "# Demonstração visual do processo\n",
        "mlp_demo = MLPBackpropagation(arquitetura=[2, 2, 1], taxa_aprendizado=0.5)\n",
        "\n",
        "print(\"\\n DEMONSTRAÇÃO DO PROCESSO:\")\n",
        "print(\"   Inicializando com pesos específicos para demonstração...\")\n",
        "\n",
        "# Pesos específicos para demonstração\n",
        "mlp_demo.pesos = [\n",
        "    np.array([[0.5, -0.5], [0.5, -0.5]]),  # Entrada → Oculta\n",
        "    np.array([[1.0], [-1.0]])              # Oculta → Saída\n",
        "]\n",
        "mlp_demo.vieses = [\n",
        "    np.array([[0.0, 0.0]]),  # Viés camada oculta\n",
        "    np.array([[0.0]])        # Viés camada saída\n",
        "]\n",
        "\n",
        "# Forward pass para uma entrada\n",
        "entrada_demo = np.array([[0, 1]])  # XOR case\n",
        "print(f\"\\n   Entrada: {entrada_demo[0]}\")\n",
        "\n",
        "saida = mlp_demo.forward(entrada_demo)\n",
        "print(f\"   Saída: {saida[0][0]:.4f}\")\n",
        "\n",
        "print(f\"\\n   Ativações camada oculta: {mlp_demo.ativacoes[1][0]}\")\n",
        "print(f\"   Somas ponderadas oculta: {mlp_demo.somas_ponderadas[0][0]}\")\n",
        "\n",
        "print(f\"\\n Backpropagation em ação:\")\n",
        "print(\"   • Calcula gradientes para cada peso\")\n",
        "print(\"   • Atualiza pesos para reduzir erro\")\n",
        "print(\"   • Repete até convergência\")\n",
        "\n",
        "\n",
        "# RESUMO HISTÓRICO E IMPORTÂNCIA\n",
        "\n",
        "\n",
        "\n",
        "print(\" IMPORTÂNCIA HISTÓRICA DO BACKPROPAGATION\")\n",
        "\n",
        "\n",
        "importancia = {\n",
        "    \"1986\": \"Rumelhart, Hinton & Williams popularizam backpropagation\",\n",
        "    \"Problema XOR\": \"Perceptrons simples não podiam resolver - MLPs com backpropagation PODIAM!\",\n",
        "    \"Inverno das RNAs\": \"Período de ceticismo (1970-1980) sobre redes neurais\",\n",
        "    \"Renascimento\": \"Backpropagation revitalizou o campo nos anos 80\",\n",
        "    \"Deep Learning\": \"Backpropagation é a base para treinar redes profundas\",\n",
        "    \"Limitações\": \"Ainda sofre com vanishing gradient em redes muito profundas\",\n",
        "    \"Evolução\": \"Leu ao desenvolvimento de LSTM, ResNet, e outras arquiteturas modernas\"\n",
        "}\n",
        "\n",
        "for ano, evento in importancia.items():\n",
        "    print(f\"    {ano}: {evento}\")\n",
        "\n",
        "print(f\"\\n LEGADO DO BACKPROPAGATION:\")\n",
        "print(f\"   • Provou que redes neurais podem aprender representações hierárquicas\")\n",
        "print(f\"   • Permitiu resolver problemas não linearmente separáveis\")\n",
        "print(f\"   • Estabeleceu a base matemática para o Deep Learning\")\n",
        "print(f\"   • Continua sendo o algoritmo fundamental para treinar redes neurais\")\n",
        "\n",
        "print(f\"\\n Backpropagation foi realmente o algoritmo que:\")\n",
        "print(f\"   'Ressuscitou as redes neurais e abriu caminho para o Deep Learning moderno!'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XkkI8Zsqfiyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tipos de Redes Neurais"
      ],
      "metadata": {
        "id": "ArT92iFfuRfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Redes Feedforward (MLP)\n",
        "\n",
        "O que é: O tipo mais básico de rede neural artificial.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Arquitetura: Conexões unidirecionais da entrada para a saída\n",
        "\n",
        "    Camadas:\n",
        "\n",
        "        Entrada: Recebe os dados\n",
        "\n",
        "        Ocultas: Processam as informações\n",
        "\n",
        "        Saída: Produz o resultado\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Classificação e regressão\n",
        "\n",
        "        Aproximação de funções\n",
        "\n",
        "        Análise de dados\n",
        "\n",
        "    Vantagens: Simples de implementar e entender\n",
        "\n",
        "    Limitações: Não mantém memória de entradas anteriores"
      ],
      "metadata": {
        "id": "e8pxyxRRuidu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MLP:\n",
        "    \"\"\"Rede Neural Multi-Layer Perceptron\"\"\"\n",
        "\n",
        "    def __init__(self, arquitetura=[2, 4, 1]):\n",
        "        # arquitetura: [entrada, oculta, saída]\n",
        "        self.arquitetura = arquitetura\n",
        "        self.inicializar_pesos()\n",
        "        print(f\"MLP criada com arquitetura: {arquitetura}\")\n",
        "\n",
        "    def inicializar_pesos(self):\n",
        "        \"\"\"Inicializa pesos aleatórios\"\"\"\n",
        "        self.pesos = []\n",
        "        self.vieses = []\n",
        "\n",
        "        for i in range(len(self.arquitetura) - 1):\n",
        "            # Inicializa pesos com valores pequenos aleatórios\n",
        "            peso_camada = np.random.randn(\n",
        "                self.arquitetura[i],\n",
        "                self.arquitetura[i+1]\n",
        "            ) * 0.1\n",
        "            vies_camada = np.zeros((1, self.arquitetura[i+1]))\n",
        "\n",
        "            self.pesos.append(peso_camada)\n",
        "            self.vieses.append(vies_camada)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Função de ativação sigmoid\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n",
        "\n",
        "    def derivada_sigmoid(self, x):\n",
        "        \"\"\"Derivada da sigmoid\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Propagação forward\"\"\"\n",
        "        self.ativacoes = [X]  # Camada de entrada\n",
        "\n",
        "        # Propaga através das camadas\n",
        "        for i in range(len(self.pesos)):\n",
        "            z = np.dot(self.ativacoes[-1], self.pesos[i]) + self.vieses[i]\n",
        "            a = self.sigmoid(z)\n",
        "            self.ativacoes.append(a)\n",
        "\n",
        "        return self.ativacoes[-1]\n",
        "\n",
        "    def backward(self, X, y, saida):\n",
        "        \"\"\"Backpropagation - calcula gradientes\"\"\"\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Erro na camada de saída\n",
        "        erro = saida - y\n",
        "        delta = erro * self.derivada_sigmoid(saida)\n",
        "\n",
        "        gradientes_pesos = []\n",
        "        gradientes_vies = []\n",
        "\n",
        "        # Calcula gradientes da última para a primeira camada\n",
        "        for i in range(len(self.pesos)-1, -1, -1):\n",
        "            grad_peso = np.dot(self.ativacoes[i].T, delta) / m\n",
        "            grad_vies = np.sum(delta, axis=0, keepdims=True) / m\n",
        "\n",
        "            gradientes_pesos.append(grad_peso)\n",
        "            gradientes_vies.append(grad_vies)\n",
        "\n",
        "            # Propaga erro para camada anterior\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.pesos[i].T) * self.derivada_sigmoid(self.ativacoes[i])\n",
        "\n",
        "        # Inverte gradientes (pois calculamos de trás pra frente)\n",
        "        return gradientes_pesos[::-1], gradientes_vies[::-1]\n",
        "\n",
        "    def treinar(self, X, y, epocas=1000, taxa_aprendizado=0.1):\n",
        "        \"\"\"Treina a rede neural\"\"\"\n",
        "        print(f\"Iniciando treinamento por {epocas} épocas...\")\n",
        "        historico_loss = []\n",
        "\n",
        "        for epoca in range(epocas):\n",
        "            # Forward pass\n",
        "            saida = self.forward(X)\n",
        "\n",
        "            # Calcula loss\n",
        "            loss = np.mean((saida - y) ** 2)\n",
        "            historico_loss.append(loss)\n",
        "\n",
        "            # Backward pass\n",
        "            grad_pesos, grad_vies = self.backward(X, y, saida)\n",
        "\n",
        "            # Atualiza pesos\n",
        "            for i in range(len(self.pesos)):\n",
        "                self.pesos[i] -= taxa_aprendizado * grad_pesos[i]\n",
        "                self.vieses[i] -= taxa_aprendizado * grad_vies[i]\n",
        "\n",
        "            # Print de progresso\n",
        "            if epoca % 200 == 0:\n",
        "                print(f\"Época {epoca}: Loss = {loss:.6f}\")\n",
        "\n",
        "        print(f\"Treinamento concluído! Loss final: {loss:.6f}\")\n",
        "        return historico_loss\n",
        "\n",
        "# Exemplo: Problema XOR (não linearmente separável)\n",
        "print(\"=== EXEMPLO MLP - PROBLEMA XOR ===\")\n",
        "\n",
        "# Dados XOR\n",
        "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_xor = np.array([[0], [1], [1], [0]])  # XOR: 1 quando as entradas são diferentes\n",
        "\n",
        "print(\"Dados de treino (XOR):\")\n",
        "for i in range(len(X_xor)):\n",
        "    print(f\"  {X_xor[i]} → {y_xor[i][0]}\")\n",
        "\n",
        "# Cria e treina MLP\n",
        "mlp = MLP(arquitetura=[2, 4, 1])  # 2 entradas, 4 neurônios ocultos, 1 saída\n",
        "historico = mlp.treinar(X_xor, y_xor, epocas=2000, taxa_aprendizado=1)\n",
        "\n",
        "# Testa o modelo\n",
        "print(\"\\n=== TESTE DO MODELO ===\")\n",
        "for i in range(len(X_xor)):\n",
        "    previsao = mlp.forward(X_xor[i:i+1])[0][0]\n",
        "    print(f\"Entrada: {X_xor[i]} → Previsão: {previsao:.4f} (Esperado: {y_xor[i][0]})\")\n",
        "\n",
        "# Plot do histórico de loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(historico)\n",
        "plt.title('Histórico de Loss durante o Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "gHU07sest0Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 Redes Convolucionais (CNNs)\n",
        "\n",
        "O que é: Redes especializadas em processar dados com estrutura grid-like (imagens).\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Características únicas:\n",
        "\n",
        "        Convoluções: Filtros que detectam características locais\n",
        "\n",
        "        Pooling: Reduz dimensionalidade mantendo características importantes\n",
        "\n",
        "        Parâmetros compartilhados: O mesmo filtro é aplicado em diferentes posições\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Reconhecimento de imagens\n",
        "\n",
        "        Processamento de vídeo\n",
        "\n",
        "        Visão computacional\n",
        "\n",
        "    Vantagens:\n",
        "\n",
        "        Invariância a translações\n",
        "\n",
        "        Eficiente com dados espaciais\n",
        "\n",
        "        Requer menos parâmetros que MLPs"
      ],
      "metadata": {
        "id": "73O5i09Zudrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"=== EXEMPLO CNN - CLASSIFICAÇÃO DE IMAGENS ===\")\n",
        "\n",
        "# Carrega dataset CIFAR-10 (imagens coloridas 32x32)\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(f\"Shape dos dados de treino: {X_train.shape}\")\n",
        "print(f\"Shape dos dados de teste: {X_test.shape}\")\n",
        "print(f\"Classes únicas: {np.unique(y_train)}\")\n",
        "\n",
        "# Nomes das classes CIFAR-10\n",
        "nomes_classes = [\n",
        "    'Avião', 'Carro', 'Pássaro', 'Gato', 'Cervo',\n",
        "    'Cachorro', 'Sapo', 'Cavalo', 'Navio', 'Caminhão'\n",
        "]\n",
        "\n",
        "# Normaliza pixels para [0, 1]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Mostra algumas imagens\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(X_train[i])\n",
        "    plt.title(nomes_classes[y_train[i][0]])\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Exemplos do Dataset CIFAR-10')\n",
        "plt.show()\n",
        "\n",
        "# Cria modelo CNN\n",
        "modelo_cnn = tf.keras.Sequential([\n",
        "    # Bloco convolucional 1\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Bloco convolucional 2\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Bloco convolucional 3\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "    # Camadas densas\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compila modelo\n",
        "modelo_cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n=== RESUMO CNN ===\")\n",
        "modelo_cnn.summary()\n",
        "\n",
        "# Treina por poucas épocas para demonstração\n",
        "print(\"\\n=== TREINAMENTO CNN ===\")\n",
        "historico_cnn = modelo_cnn.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5,  # Poucas épocas para demonstração\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliação\n",
        "print(\"\\n=== AVALIAÇÃO ===\")\n",
        "test_loss, test_accuracy = modelo_cnn.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Acurácia no teste: {test_accuracy:.4f}\")\n",
        "\n",
        "# Previsões de exemplo\n",
        "print(\"\\n=== PREVISÕES DE EXEMPLO ===\")\n",
        "indices_exemplo = [0, 1, 2, 3, 4]\n",
        "previsoes = modelo_cnn.predict(X_test[indices_exemplo])\n",
        "classes_previstas = np.argmax(previsoes, axis=1)\n",
        "\n",
        "print(\"Previsões para 5 primeiras imagens de teste:\")\n",
        "for i, idx in enumerate(indices_exemplo):\n",
        "    real = nomes_classes[y_test[idx][0]]\n",
        "    previsto = nomes_classes[classes_previstas[i]]\n",
        "    confianca = previsoes[i][classes_previstas[i]]\n",
        "    print(f\"  Real: {real:10} → Previsto: {previsto:10} (Confiança: {confianca:.2f})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zpTNG0Twuo0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Redes Recorrentes (RNNs/LSTMs)\n",
        "\n",
        "O que é: Redes com loops que permitem manter informação temporal.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Funcionamento:\n",
        "\n",
        "        Mantém um \"estado oculto\" que funciona como memória\n",
        "\n",
        "        Processa sequências elemento por elemento\n",
        "\n",
        "    Tipos:\n",
        "\n",
        "        RNN Simples: Memória de curto prazo\n",
        "\n",
        "        LSTM (Long Short-Term Memory): Memória de longo e curto prazo com portas\n",
        "\n",
        "        GRU (Gated Recurrent Unit): Versão simplificada da LSTM\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Processamento de linguagem natural\n",
        "\n",
        "        Análise de séries temporais\n",
        "\n",
        "        Reconhecimento de fala"
      ],
      "metadata": {
        "id": "XwZCdP_curMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RNNSimples:\n",
        "    \"\"\"Implementação simplificada de RNN para séries temporais\"\"\"\n",
        "\n",
        "    def __init__(self, unidades_ocultas, dimensao_entrada):\n",
        "        # Número de unidades na camada oculta\n",
        "        self.unidades_ocultas = unidades_ocultas\n",
        "        # Dimensão dos dados de entrada\n",
        "        self.dimensao_entrada = dimensao_entrada\n",
        "\n",
        "        # Inicialização de pesos com valores pequenos\n",
        "        # W_xh: pesos da entrada para estado oculto (forma: entrada x oculto)\n",
        "        self.W_xh = np.random.randn(dimensao_entrada, unidades_ocultas) * 0.01\n",
        "        # W_hh: pesos do estado oculto anterior para estado oculto atual (forma: oculto x oculto)\n",
        "        self.W_hh = np.random.randn(unidades_ocultas, unidades_ocultas) * 0.01\n",
        "        # W_hy: pesos do estado oculto para saída (forma: oculto x saída)\n",
        "        self.W_hy = np.random.randn(unidades_ocultas, dimensao_entrada) * 0.01\n",
        "\n",
        "        # Inicialização de vieses:\n",
        "        self.b_h = np.zeros((1, unidades_ocultas))  # Viés do estado oculto\n",
        "        self.b_y = np.zeros((1, dimensao_entrada))  # Viés da saída\n",
        "\n",
        "        print(f\"RNN criada: {dimensao_entrada} entradas → {unidades_ocultas} unidades ocultas\")\n",
        "\n",
        "    def passo_forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Um passo forward da RNN\n",
        "        x: entrada no passo atual (forma: 1 x dimensao_entrada)\n",
        "        h_prev: estado oculto anterior (forma: 1 x unidades_ocultas)\n",
        "        \"\"\"\n",
        "        # Calcula novo estado oculto: h = tanh(W_xh * x + W_hh * h_prev + b_h)\n",
        "        h = np.tanh(np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)\n",
        "\n",
        "        # Calcula saída: y = W_hy * h + b_y\n",
        "        y = np.dot(h, self.W_hy) + self.b_y\n",
        "\n",
        "        return h, y\n",
        "\n",
        "    def forward_sequence(self, X):\n",
        "        \"\"\"\n",
        "        Propaga uma sequência completa através da RNN\n",
        "        X: sequência de entrada (forma: comprimento_sequencia x dimensao_entrada)\n",
        "        \"\"\"\n",
        "        # Inicializa estado oculto com zeros\n",
        "        h = np.zeros((1, self.unidades_ocultas))\n",
        "        self.historico_h = [h]  # Armazena histórico de estados ocultos\n",
        "        saidas = []  # Lista para armazenar saídas\n",
        "\n",
        "        # Itera através de cada passo de tempo da sequência\n",
        "        for x in X:\n",
        "            x = x.reshape(1, -1)  # Garante que x tem shape (1, dimensao_entrada)\n",
        "            # Calcula novo estado oculto e saída\n",
        "            h, y = self.passo_forward(x, h)\n",
        "            self.historico_h.append(h)\n",
        "            saidas.append(y)\n",
        "\n",
        "        return np.array(saidas)\n",
        "\n",
        "# Exemplo de uso da RNN Simples\n",
        "print(\"=== EXEMPLO RNN SIMPLES - SÉRIE TEMPORAL SINTÉTICA ===\")\n",
        "\n",
        "# Cria dados sintéticos: sequência senoidal com ruído\n",
        "comprimento_sequencia = 20\n",
        "dimensao_entrada = 1\n",
        "\n",
        "# Gera sequência senoidal\n",
        "t = np.linspace(0, 4*np.pi, comprimento_sequencia)\n",
        "X_sequence = np.sin(t).reshape(-1, 1) + np.random.normal(0, 0.1, comprimento_sequencia).reshape(-1, 1)\n",
        "\n",
        "print(f\"Sequência de entrada ({comprimento_sequencia} passos):\")\n",
        "print([f\"{x[0]:.3f}\" for x in X_sequence])\n",
        "\n",
        "# Cria e testa RNN\n",
        "rnn = RNNSimples(unidades_ocultas=10, dimensao_entrada=dimensao_entrada)\n",
        "\n",
        "# Propaga a sequência\n",
        "saidas = rnn.forward_sequence(X_sequence)\n",
        "\n",
        "print(f\"\\nSaída da RNN ({len(saidas)} passos):\")\n",
        "print([f\"{s[0][0]:.3f}\" for s in saidas])\n",
        "\n",
        "# Visualiza estados ocultos\n",
        "print(f\"\\nEstados ocultos (forma: {rnn.historico_h[1].shape}):\")\n",
        "print(f\"Primeiro estado oculto: {[f'{v:.3f}' for v in rnn.historico_h[1][0]]}\")\n",
        "\n",
        "# Plot dos resultados\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(t, X_sequence, 'bo-', label='Entrada', markersize=4)\n",
        "plt.plot(t, saidas.reshape(-1), 'ro-', label='Saída RNN', markersize=4)\n",
        "plt.title('RNN Simples - Entrada vs Saída')\n",
        "plt.xlabel('Tempo')\n",
        "plt.ylabel('Valor')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Visualiza alguns estados ocultos\n",
        "estados_plot = np.array([h.flatten() for h in rnn.historico_h[1:6]]).T\n",
        "for i in range(min(5, rnn.unidades_ocultas)):\n",
        "    plt.plot(range(5), estados_plot[i], label=f'Neurônio {i+1}')\n",
        "plt.title('Evolução dos Estados Ocultos\\n(Primeiros 5 neurônios)')\n",
        "plt.xlabel('Passo de Tempo')\n",
        "plt.ylabel('Valor de Ativação')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "SAÍDA ESPERADA:\n",
        "\n",
        "=== EXEMPLO RNN SIMPLES - SÉRIE TEMPORAL SINTÉTICA ===\n",
        "Sequência de entrada (20 passos):\n",
        "['0.123', '0.345', '0.567', '0.789', '0.901', '0.987', '0.845', '0.678', ...]\n",
        "\n",
        "RNN criada: 1 entradas → 10 unidades ocultas\n",
        "\n",
        "Saída da RNN (20 passos):\n",
        "['0.012', '0.034', '0.045', '0.067', '0.078', '0.089', '0.076', '0.065', ...]\n",
        "\n",
        "Estados ocultos (forma: (1, 10)):\n",
        "Primeiro estado oculto: ['0.012', '-0.023', '0.045', '-0.067', '0.089', ...]\n",
        "\"\"\"\n",
        "\n",
        "# Exemplo com TensorFlow/Keras - LSTM para previsão de séries temporais\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== EXEMPLO LSTM COM TENSORFLOW - PREVISÃO DE SÉRIES TEMPORAIS ===\")\n",
        "\n",
        "def criar_lstm_sequence():\n",
        "    \"\"\"Cria modelo LSTM para previsão de séries temporais\"\"\"\n",
        "    modelo = models.Sequential([\n",
        "        # Primeira camada LSTM com 50 unidades\n",
        "        # return_sequences=True retorna saída para cada passo de tempo\n",
        "        layers.LSTM(50, activation='relu', return_sequences=True,\n",
        "                   input_shape=(10, 1)),  # 10 time steps, 1 feature\n",
        "        # Segunda camada LSTM\n",
        "        layers.LSTM(50, activation='relu'),\n",
        "        # Camada densa de saída com 1 neurônio (previsão de valor único)\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    # Compila modelo para regressão\n",
        "    modelo.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return modelo\n",
        "\n",
        "# Cria dados de exemplo para previsão de série temporal\n",
        "def criar_dados_serie_temporal(n_amostras=1000, comprimento_sequencia=10):\n",
        "    \"\"\"Cria dados sintéticos de série temporal\"\"\"\n",
        "    # Gera série temporal com tendência + sazonalidade + ruído\n",
        "    t = np.linspace(0, 20, n_amostras + comprimento_sequencia)\n",
        "    serie = np.sin(t) + 0.5 * np.sin(2*t) + 0.1 * np.random.randn(len(t))\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(n_amostras):\n",
        "        # Janela deslizante: usa 10 passos para prever próximo valor\n",
        "        X.append(serie[i:i+comprimento_sequencia])\n",
        "        y.append(serie[i+comprimento_sequencia])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Prepara dados\n",
        "X, y = criar_dados_serie_temporal()\n",
        "X = X.reshape(-1, 10, 1)  # Formato para LSTM: (amostras, timesteps, features)\n",
        "\n",
        "print(f\"Shape dos dados: X {X.shape}, y {y.shape}\")\n",
        "\n",
        "# Divide em treino e teste\n",
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Treino: {X_train.shape}, Teste: {X_test.shape}\")\n",
        "\n",
        "# Cria e treina modelo LSTM\n",
        "modelo_lstm = criar_lstm_sequence()\n",
        "\n",
        "print(\"\\n=== RESUMO DO MODELO LSTM ===\")\n",
        "modelo_lstm.summary()\n",
        "\n",
        "# Treina o modelo\n",
        "print(\"\\n=== TREINANDO LSTM ===\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Avaliação\n",
        "print(\"\\n=== AVALIAÇÃO ===\")\n",
        "test_loss, test_mae = modelo_lstm.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Loss no teste: {test_loss:.4f}\")\n",
        "print(f\"MAE no teste: {test_mae:.4f}\")\n",
        "\n",
        "# Previsões\n",
        "print(\"\\n=== EXEMPLO DE PREVISÕES ===\")\n",
        "previsoes = modelo_lstm.predict(X_test[:10])\n",
        "print(\"Comparação: Real vs Previsto\")\n",
        "for i in range(5):\n",
        "    print(f\"  Real: {y_test[i]:7.4f} → Previsto: {previsoes[i][0]:7.4f}\")\n",
        "\n",
        "# Visualização\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Curvas de treino\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(historico.history['loss'], label='Treino')\n",
        "plt.plot(historico.history['val_loss'], label='Validação')\n",
        "plt.title('Loss durante Treinamento')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Série temporal original vs previsões\n",
        "plt.subplot(1, 3, 2)\n",
        "# Pega um trecho da série para visualizar\n",
        "trecho_test = 100\n",
        "indices = range(split_idx, split_idx + trecho_test)\n",
        "\n",
        "# Previsões para o trecho de teste\n",
        "previsoes_trecho = modelo_lstm.predict(X_test[:trecho_test])\n",
        "\n",
        "plt.plot(indices, y_test[:trecho_test], 'b-', label='Real', alpha=0.7)\n",
        "plt.plot(indices, previsoes_trecho.flatten(), 'r-', label='Previsto', alpha=0.7)\n",
        "plt.title('Previsões LSTM vs Valores Reais')\n",
        "plt.xlabel('Tempo')\n",
        "plt.ylabel('Valor')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Zoom em uma parte específica\n",
        "plt.subplot(1, 3, 3)\n",
        "zoom_trecho = 30\n",
        "plt.plot(y_test[:zoom_trecho], 'bo-', label='Real', markersize=4)\n",
        "plt.plot(previsoes_trecho.flatten()[:zoom_trecho], 'ro-', label='Previsto', markersize=4)\n",
        "plt.title('Zoom - Previsões vs Real')\n",
        "plt.xlabel('Passo de Tempo')\n",
        "plt.ylabel('Valor')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Exemplo de previsão multi-step\n",
        "print(\"\\n=== PREVISÃO MULTI-STEP ===\")\n",
        "def previsao_multi_step(modelo, sequencia_inicial, n_passos=5):\n",
        "    \"\"\"Faz previsão multi-step usando a própria previsão como entrada\"\"\"\n",
        "    sequencia_atual = sequencia_inicial.copy()\n",
        "    previsoes = []\n",
        "\n",
        "    for _ in range(n_passos):\n",
        "        # Faz previsão para próximo passo\n",
        "        previsao = modelo.predict(sequencia_atual.reshape(1, 10, 1), verbose=0)[0][0]\n",
        "        previsoes.append(previsao)\n",
        "\n",
        "        # Atualiza sequência: remove primeiro elemento, adiciona previsão\n",
        "        sequencia_atual = np.roll(sequencia_atual, -1)\n",
        "        sequencia_atual[-1] = previsao\n",
        "\n",
        "    return previsoes\n",
        "\n",
        "# Testa previsão multi-step\n",
        "sequencia_teste = X_test[0].flatten()\n",
        "previsoes_multi = previsao_multi_step(modelo_lstm, sequencia_teste, n_passos=5)\n",
        "\n",
        "print(\"Previsão Multi-Step:\")\n",
        "print(\"Sequência inicial:\", [f\"{x:.3f}\" for x in sequencia_teste])\n",
        "print(\"Previsões:\", [f\"{p:.3f}\" for p in previsoes_multi])\n",
        "print(\"Valores reais seguintes:\", [f\"{y:.3f}\" for y in y_test[1:6]])\n",
        "\n"
      ],
      "metadata": {
        "id": "cCm6loCiuvfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Autoencoders\n",
        "\n",
        "O que é: Redes não supervisionadas para aprendizado de representações.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Estrutura:\n",
        "\n",
        "        Encoder: Comprime a entrada em uma representação latente\n",
        "\n",
        "        Decoder: Reconstrói a entrada a partir da representação latente\n",
        "\n",
        "    Objetivo: Aprender uma representação compacta e significativa dos dados\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Redução de dimensionalidade\n",
        "\n",
        "        Denoising (remoção de ruído)\n",
        "\n",
        "        Geração de dados\n",
        "\n",
        "        Detecção de anomalias"
      ],
      "metadata": {
        "id": "d1egqy1huuY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs, load_digits\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "class Autoencoder:\n",
        "    \"\"\"Autoencoder para redução de dimensionalidade e aprendizado de representações\"\"\"\n",
        "\n",
        "    def __init__(self, dimensao_entrada, dimensao_latente):\n",
        "        \"\"\"\n",
        "        Inicializa o Autoencoder\n",
        "\n",
        "        Args:\n",
        "            dimensao_entrada (int): Número de features dos dados de entrada\n",
        "            dimensao_latente (int): Dimensão do espaço latente (codificação comprimida)\n",
        "        \"\"\"\n",
        "        # Dimensão dos dados de entrada (ex: 64 pixels para imagens 8x8)\n",
        "        self.dimensao_entrada = dimensao_entrada\n",
        "        # Dimensão do espaço latente (representação comprimida) - geralmente 2-50\n",
        "        self.dimensao_latente = dimensao_latente\n",
        "\n",
        "        print(f\"Criando Autoencoder: {dimensao_entrada} → {dimensao_latente} → {dimensao_entrada}\")\n",
        "\n",
        "        # Encoder: comprime a entrada para o espaço latente\n",
        "        self.encoder = models.Sequential([\n",
        "            # Primeira camada: reduz para 128 neurônios com ativação ReLU\n",
        "            layers.Dense(128, activation='relu', input_shape=(dimensao_entrada,)),\n",
        "            # Segunda camada: reduz para 64 neurônios\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            # Camada latente: representa os dados comprimidos (bottleneck)\n",
        "            layers.Dense(dimensao_latente, activation='relu', name='bottleneck')\n",
        "        ])\n",
        "\n",
        "        # Decoder: reconstroi os dados do espaço latente\n",
        "        self.decoder = models.Sequential([\n",
        "            # Primeira camada: expande para 64 neurônios\n",
        "            layers.Dense(64, activation='relu', input_shape=(dimensao_latente,)),\n",
        "            # Segunda camada: expande para 128 neurônios\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            # Camada de saída: reconstroi para dimensão original com sigmoid (valores 0-1)\n",
        "            layers.Dense(dimensao_entrada, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Autoencoder completo: encoder + decoder\n",
        "        self.modelo = models.Sequential([self.encoder, self.decoder])\n",
        "\n",
        "        # Compila com MSE pois queremos reconstruir a entrada (comparação pixel a pixel)\n",
        "        self.modelo.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        print(\" Autoencoder criado e compilado com sucesso!\")\n",
        "\n",
        "    def treinar(self, X, epocas=50, batch_size=32):\n",
        "        \"\"\"\n",
        "        Treina o autoencoder para reconstruir sua própria entrada\n",
        "\n",
        "        Args:\n",
        "            X (array): Dados de entrada\n",
        "            epocas (int): Número de épocas de treinamento\n",
        "            batch_size (int): Tamanho do lote\n",
        "\n",
        "        Returns:\n",
        "            historico: Histórico do treinamento\n",
        "        \"\"\"\n",
        "        print(f\" Iniciando treinamento por {epocas} épocas...\")\n",
        "        print(f\"   Dados: {X.shape[0]} amostras, {X.shape[1]} features\")\n",
        "\n",
        "        # Treina o autoencoder: entrada = saída esperada (reconstrução)\n",
        "        historico = self.modelo.fit(\n",
        "            X, X,  #  Entrada = Saída esperada (aprendizado não supervisionado)\n",
        "            epochs=epocas,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.2,  # 20% dos dados para validação\n",
        "            verbose=1,  # Mostra progresso\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "        )\n",
        "\n",
        "        print(\" Treinamento concluído!\")\n",
        "        return historico\n",
        "\n",
        "    def codificar(self, X):\n",
        "        \"\"\"\n",
        "        Converte dados para o espaço latente (redução de dimensionalidade)\n",
        "\n",
        "        Args:\n",
        "            X (array): Dados de entrada\n",
        "\n",
        "        Returns:\n",
        "            array: Representação latente dos dados\n",
        "        \"\"\"\n",
        "        return self.encoder.predict(X, verbose=0)\n",
        "\n",
        "    def decodificar(self, Z):\n",
        "        \"\"\"\n",
        "        Reconstrói dados a partir do espaço latente\n",
        "\n",
        "        Args:\n",
        "            Z (array): Dados no espaço latente\n",
        "\n",
        "        Returns:\n",
        "            array: Dados reconstruídos\n",
        "        \"\"\"\n",
        "        return self.decoder.predict(Z, verbose=0)\n",
        "\n",
        "    def reconstruir(self, X):\n",
        "        \"\"\"\n",
        "        Reconstroi os dados de entrada passando pelo espaço latente\n",
        "\n",
        "        Args:\n",
        "            X (array): Dados de entrada\n",
        "\n",
        "        Returns:\n",
        "            array: Dados reconstruídos\n",
        "        \"\"\"\n",
        "        return self.modelo.predict(X, verbose=0)\n",
        "\n",
        "\n",
        "# EXEMPLO 1: Autoencoder para Dados Sintéticos\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 1: AUTOENCODER PARA DADOS SINTÉTICOS\")\n",
        "\n",
        "# Cria dados sintéticos 3D com estrutura clara\n",
        "X_sintetico, y_sintetico = make_blobs(\n",
        "    n_samples=1000,\n",
        "    n_features=10,  # 10 dimensões\n",
        "    centers=3,      # 3 clusters\n",
        "    random_state=42,\n",
        "    cluster_std=0.8\n",
        ")\n",
        "\n",
        "# Normaliza os dados entre 0 e 1\n",
        "scaler = MinMaxScaler()\n",
        "X_sintetico_normalizado = scaler.fit_transform(X_sintetico)\n",
        "\n",
        "print(f\" Dados sintéticos: {X_sintetico_normalizado.shape}\")\n",
        "print(f\" Clusters: {np.unique(y_sintetico)}\")\n",
        "\n",
        "# Cria e treina autoencoder para reduzir de 10D para 2D\n",
        "autoencoder_sintetico = Autoencoder(\n",
        "    dimensao_entrada=10,\n",
        "    dimensao_latente=2  # Reduzindo para 2D para visualização\n",
        ")\n",
        "\n",
        "# Treina o modelo\n",
        "historico_sintetico = autoencoder_sintetico.treinar(\n",
        "    X_sintetico_normalizado,\n",
        "    epocas=100,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Codifica os dados para o espaço latente\n",
        "X_latente = autoencoder_sintetico.codificar(X_sintetico_normalizado)\n",
        "\n",
        "print(f\" Dimensões reduzidas: {X_sintetico_normalizado.shape[1]}D → {X_latente.shape[1]}D\")\n",
        "\n",
        "# Reconstroi os dados\n",
        "X_reconstruido = autoencoder_sintetico.reconstruir(X_sintetico_normalizado)\n",
        "\n",
        "# Calcula erro de reconstrução\n",
        "erro_reconstrucao = np.mean((X_sintetico_normalizado - X_reconstruido) ** 2)\n",
        "print(f\" Erro médio de reconstrução: {erro_reconstrucao:.6f}\")\n",
        "\n",
        "# Visualização dos resultados\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Gráfico 1: Loss durante treinamento\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(historico_sintetico.history['loss'], label='Treino')\n",
        "plt.plot(historico_sintetico.history['val_loss'], label='Validação')\n",
        "plt.title('Loss do Autoencoder\\n(Dados Sintéticos 10D → 2D)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: Espaço latente 2D colorido por cluster\n",
        "plt.subplot(1, 3, 2)\n",
        "scatter = plt.scatter(X_latente[:, 0], X_latente[:, 1], c=y_sintetico, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title('Espaço Latente 2D\\n(Representação Comprimida)')\n",
        "plt.xlabel('Componente Latente 1')\n",
        "plt.ylabel('Componente Latente 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 3: Comparação com PCA tradicional\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_sintetico_normalizado)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "scatter_pca = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_sintetico, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter_pca, label='Cluster')\n",
        "plt.title('PCA 2D\\n(Comparação)')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 2: Autoencoder para Dígitos Manuscritos (MNIST)\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 2: AUTOENCODER PARA DÍGITOS MANUSCRITOS\")\n",
        "\n",
        "\n",
        "# Carrega dataset de dígitos (imagens 8x8 = 64 pixels)\n",
        "digitos = load_digits()\n",
        "X_digitos = digitos.data.astype('float32') / 16.0  # Normaliza para 0-1 (valores originais 0-16)\n",
        "y_digitos = digitos.target\n",
        "\n",
        "print(f\"Dataset Dígitos: {X_digitos.shape}\")\n",
        "print(f\"Dígitos únicos: {np.unique(y_digitos)}\")\n",
        "print(f\"Dimensão das imagens: 8x8 = 64 pixels\")\n",
        "\n",
        "# Cria autoencoder para imagens de dígitos\n",
        "autoencoder_digitos = Autoencoder(\n",
        "    dimensao_entrada=64,   # 8x8 pixels\n",
        "    dimensao_latente=10    # Comprime para 10 dimensões\n",
        ")\n",
        "\n",
        "# Treina o modelo\n",
        "historico_digitos = autoencoder_digitos.treinar(\n",
        "    X_digitos,\n",
        "    epocas=100,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Codifica e decodifica algumas imagens de exemplo\n",
        "indices_exemplo = [0, 1, 100, 101]  # Índices para mostrar exemplos\n",
        "X_exemplo = X_digitos[indices_exemplo]\n",
        "\n",
        "# Reconstroi as imagens\n",
        "X_reconstruido_digitos = autoencoder_digitos.reconstruir(X_exemplo)\n",
        "\n",
        "# Codifica para espaço latente\n",
        "X_latente_digitos = autoencoder_digitos.codificar(X_exemplo)\n",
        "\n",
        "print(f\"\\n Exemplo de codificação latente:\")\n",
        "for i, idx in enumerate(indices_exemplo):\n",
        "    print(f\"  Dígito {y_digitos[idx]}: {X_latente_digitos[i]}\")\n",
        "\n",
        "# Visualização das reconstruções\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Gráfico 1: Loss durante treinamento\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.plot(historico_digitos.history['loss'], label='Treino')\n",
        "plt.plot(historico_digitos.history['val_loss'], label='Validação')\n",
        "plt.title('Loss do Autoencoder\\n(Dígitos 64D → 10D)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Mostra imagens originais vs reconstruídas\n",
        "for i, idx in enumerate(indices_exemplo):\n",
        "    # Imagem original\n",
        "    plt.subplot(2, 4, i + 2)\n",
        "    plt.imshow(X_exemplo[i].reshape(8, 8), cmap='gray')\n",
        "    plt.title(f'Original: {y_digitos[idx]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Imagem reconstruída\n",
        "    plt.subplot(2, 4, i + 6)\n",
        "    plt.imshow(X_reconstruido_digitos[i].reshape(8, 8), cmap='gray')\n",
        "    plt.title(f'Reconstruído')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 3: Visualização do Espaço Latente\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 3: ANÁLISE DO ESPAÇO LATENTE\")\n",
        "\n",
        "\n",
        "# Codifica todo o dataset para o espaço latente 2D\n",
        "autoencoder_2d = Autoencoder(dimensao_entrada=64, dimensao_latente=2)\n",
        "historico_2d = autoencoder_2d.treinar(X_digitos, epocas=150, batch_size=128)\n",
        "\n",
        "# Codifica todos os dígitos para 2D\n",
        "X_latente_completo = autoencoder_2d.codificar(X_digitos)\n",
        "\n",
        "print(f\" Espaço latente completo: {X_latente_completo.shape}\")\n",
        "\n",
        "# Visualização do espaço latente 2D\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Gráfico 1: Espaço latente colorido por dígito\n",
        "plt.subplot(1, 3, 1)\n",
        "scatter = plt.scatter(X_latente_completo[:, 0], X_latente_completo[:, 1],\n",
        "                     c=y_digitos, cmap='tab10', alpha=0.7, s=30)\n",
        "plt.colorbar(scatter, label='Dígito')\n",
        "plt.title('Espaço Latente 2D - Dígitos MNIST\\n(Colorido por classe)')\n",
        "plt.xlabel('Componente Latente 1')\n",
        "plt.ylabel('Componente Latente 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Gráfico 2: Grid no espaço latente - gerando novos dígitos\n",
        "plt.subplot(1, 3, 2)\n",
        "\n",
        "# Cria grid no espaço latente\n",
        "grid_size = 10\n",
        "x = np.linspace(X_latente_completo[:, 0].min(), X_latente_completo[:, 0].max(), grid_size)\n",
        "y = np.linspace(X_latente_completo[:, 1].min(), X_latente_completo[:, 1].max(), grid_size)\n",
        "XX, YY = np.meshgrid(x, y)\n",
        "\n",
        "# Decodifica cada ponto do grid\n",
        "grid_imagens = []\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        ponto_latente = np.array([[XX[i, j], YY[i, j]]])\n",
        "        imagem_gerada = autoencoder_2d.decodificar(ponto_latente)\n",
        "        grid_imagens.append(imagem_gerada.reshape(8, 8))\n",
        "\n",
        "# Cria mosaico com as imagens geradas\n",
        "mosaico = np.zeros((8 * grid_size, 8 * grid_size))\n",
        "for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "        mosaico[i * 8:(i + 1) * 8, j * 8:(j + 1) * 8] = grid_imagens[i * grid_size + j]\n",
        "\n",
        "plt.imshow(mosaico, cmap='gray', extent=[\n",
        "    X_latente_completo[:, 0].min(),\n",
        "    X_latente_completo[:, 0].max(),\n",
        "    X_latente_completo[:, 1].min(),\n",
        "    X_latente_completo[:, 1].max()\n",
        "])\n",
        "plt.title('Grid do Espaço Latente\\n(Geração de Novos Dígitos)')\n",
        "plt.xlabel('Componente Latente 1')\n",
        "plt.ylabel('Componente Latente 2')\n",
        "\n",
        "# Gráfico 3: Comparação de reconstruções\n",
        "plt.subplot(1, 3, 3)\n",
        "\n",
        "# Seleciona alguns dígitos diferentes\n",
        "dígitos_para_comparar = [0, 1, 2, 3]\n",
        "cores = ['red', 'blue', 'green', 'orange']\n",
        "\n",
        "for i, digito in enumerate(dígitos_para_comparar):\n",
        "    # Filtra dígitos específicos\n",
        "    mascara = y_digitos == digito\n",
        "    X_digito = X_latente_completo[mascara]\n",
        "\n",
        "    plt.scatter(X_digito[:, 0], X_digito[:, 1],\n",
        "               color=cores[i], label=f'Dígito {digito}', alpha=0.6, s=20)\n",
        "\n",
        "plt.title('Espaço Latente - Agrupamento por Dígito')\n",
        "plt.xlabel('Componente Latente 1')\n",
        "plt.ylabel('Componente Latente 2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 4: Denoising Autoencoder\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 4: DENOISING AUTOENCODER\")\n",
        "\n",
        "\n",
        "def adicionar_ruido(X, nivel_ruido=0.5):\n",
        "    \"\"\"Adiciona ruído aleatório aos dados\"\"\"\n",
        "    ruido = np.random.normal(0, nivel_ruido, X.shape)\n",
        "    X_ruidoso = X + ruido\n",
        "    # Garante que os valores permaneçam entre 0 e 1\n",
        "    return np.clip(X_ruidoso, 0, 1)\n",
        "\n",
        "# Cria versões ruidosas dos dígitos\n",
        "X_digitos_ruidoso = adicionar_ruido(X_digitos, nivel_ruido=0.3)\n",
        "\n",
        "print(\"Adicionando ruído aos dados...\")\n",
        "\n",
        "# Cria denoising autoencoder (mesma arquitetura)\n",
        "denoiser = Autoencoder(dimensao_entrada=64, dimensao_latente=16)\n",
        "\n",
        "# Treina para remover ruído: entrada ruidosa → saída limpa\n",
        "historico_denoiser = denoiser.treinar(\n",
        "    X_digitos_ruidoso,  # Entrada: dados com ruído\n",
        "    X_digitos,          # Saída: dados originais sem ruído\n",
        "    epocas=100,\n",
        "    batch_size=128\n",
        ")\n",
        "\n",
        "# Testa o denoiser\n",
        "indices_teste = [10, 11, 12, 13]\n",
        "X_teste_ruidoso = X_digitos_ruidoso[indices_teste]\n",
        "X_teste_reconstruido = denoiser.reconstruir(X_teste_ruidoso)\n",
        "\n",
        "# Visualização dos resultados de denoising\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(historico_denoiser.history['loss'], label='Treino')\n",
        "plt.plot(historico_denoiser.history['val_loss'], label='Validação')\n",
        "plt.title('Loss - Denoising Autoencoder')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "for i, idx in enumerate(indices_teste):\n",
        "    plt.subplot(1, 4, i + 2)\n",
        "\n",
        "    # Mostra: original → ruidoso → reconstruído\n",
        "    if i == 0:\n",
        "        plt.imshow(X_teste_reconstruido[i].reshape(8, 8), cmap='gray')\n",
        "        plt.title(f'Reconstruído\\n(Dígito {y_digitos[idx]})')\n",
        "    else:\n",
        "        plt.imshow(X_teste_reconstruido[i].reshape(8, 8), cmap='gray')\n",
        "        plt.title(f'Reconstruído')\n",
        "\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Métricas de qualidade\n",
        "mse_ruidoso = np.mean((X_digitos[indices_teste] - X_teste_ruidoso) ** 2)\n",
        "mse_reconstruido = np.mean((X_digitos[indices_teste] - X_teste_reconstruido) ** 2)\n",
        "\n",
        "print(f\" Métricas de Denoising:\")\n",
        "print(f\"   MSE dados ruidosos: {mse_ruidoso:.6f}\")\n",
        "print(f\"   MSE após reconstrução: {mse_reconstruido:.6f}\")\n",
        "print(f\"    Melhoria: {((mse_ruidoso - mse_reconstruido) / mse_ruidoso * 100):.1f}%\")\n",
        "\n",
        "\n",
        "# RESUMO E APLICAÇÕES PRÁTICAS\n",
        "#\n",
        "\n",
        "\n",
        "print(\" RESUMO: APLICAÇÕES PRÁTICAS DO AUTOENCODER\")\n",
        "\n",
        "\n",
        "aplicacoes = {\n",
        "    \"1. Redução de Dimensionalidade\": \"Compressão de dados mantendo informações importantes\",\n",
        "    \"2. Detecção de Anomalias\": \"Dados com alta perda na reconstrução são possíveis anomalias\",\n",
        "    \"3. Denoising\": \"Remoção de ruído de imagens ou sinais\",\n",
        "    \"4. Geração de Dados\": \"Criação de novos exemplos a partir do espaço latente\",\n",
        "    \"5. Feature Learning\": \"Aprendizado de representações úteis para outras tarefas\",\n",
        "    \"6. Visualização\": \"Projeção de dados de alta dimensão em 2D/3D\"\n",
        "}\n",
        "\n",
        "for aplicacao, descricao in aplicacoes.items():\n",
        "    print(f\"   {aplicacao}: {descricao}\")\n",
        "\n",
        "print(f\"\\n Dicas Práticas:\")\n",
        "print(f\"   • Dimensão latente: Comece com 2-50 para visualização, 10-500 para compressão\")\n",
        "print(f\"   • Arquitetura: Encoder simétrico ao decoder geralmente funciona bem\")\n",
        "print(f\"   • Ativações: Use ReLU para camadas internas, sigmoid/tanh para saída\")\n",
        "print(f\"   • Regularização: Dropout e early stopping ajudam a evitar overfitting\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9n30xfNbu0pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 Redes Generativas (GANs)\n",
        "\n",
        "O que é: Arquitetura que gera novos dados similares aos de treinamento.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Componentes:\n",
        "\n",
        "        Gerador: Cria dados falsos a partir de ruído\n",
        "\n",
        "        Discriminador: Distingue entre dados reais e falsos\n",
        "\n",
        "    Treinamento: Os dois componentes competem em um jogo minimax\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Geração de imagens\n",
        "\n",
        "        Aumento de dados\n",
        "\n",
        "        Transferência de estilo\n",
        "\n",
        "        Criação de arte"
      ],
      "metadata": {
        "id": "o1uUghSEu4mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "class GANSimples:\n",
        "    \"\"\"Implementação simplificada de Generative Adversarial Network\"\"\"\n",
        "\n",
        "    def __init__(self, dimensao_latente, dimensao_entrada):\n",
        "        \"\"\"\n",
        "        Inicializa a GAN com gerador e discriminador\n",
        "\n",
        "        Args:\n",
        "            dimensao_latente (int): Dimensão do vetor de ruído (entrada do gerador)\n",
        "            dimensao_entrada (int): Dimensão dos dados reais (saída do gerador)\n",
        "        \"\"\"\n",
        "        # Dimensão do vetor de ruído (espaço latente) - tipicamente 50-200\n",
        "        self.dimensao_latente = dimensao_latente\n",
        "        # Dimensão dos dados reais que queremos gerar\n",
        "        self.dimensao_entrada = dimensao_entrada\n",
        "\n",
        "        print(f\"Inicializando GAN: Ruído {dimensao_latente}D → Dados {dimensao_entrada}D\")\n",
        "\n",
        "        # Constrói os componentes da GAN\n",
        "        self.construir_gerador()\n",
        "        self.construir_discriminador()\n",
        "        self.construir_gan()\n",
        "\n",
        "        # Para acompanhar o progresso do treinamento\n",
        "        self.historico_loss_d = []\n",
        "        self.historico_loss_g = []\n",
        "        self.historico_acc_d = []\n",
        "\n",
        "    def construir_gerador(self):\n",
        "        \"\"\"Constrói o gerador que transforma ruído em dados falsos\"\"\"\n",
        "        print(\" Construindo Gerador...\")\n",
        "\n",
        "        self.gerador = models.Sequential([\n",
        "            # Primeira camada: expande o ruído para 128 neurônios\n",
        "            layers.Dense(128, activation='relu',\n",
        "                        input_shape=(self.dimensao_latente,)),\n",
        "            layers.BatchNormalization(),  # Normaliza as ativações - ajuda na estabilidade\n",
        "\n",
        "            # Segunda camada: expande para 256 neurônios\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "\n",
        "            # Terceira camada: expande para 512 neurônios\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "\n",
        "            # Camada de saída: gera dados com mesma dimensão dos dados reais\n",
        "            # tanh produz valores entre -1 e 1 (adequado para dados normalizados)\n",
        "            layers.Dense(self.dimensao_entrada, activation='tanh')\n",
        "        ])\n",
        "\n",
        "        print(f\"    Gerador criado: {self.dimensao_latente} → 128 → 256 → 512 → {self.dimensao_entrada}\")\n",
        "\n",
        "    def construir_discriminador(self):\n",
        "        \"\"\"Constrói o discriminador que classifica dados como reais ou falsos\"\"\"\n",
        "        print(\" Construindo Discriminador...\")\n",
        "\n",
        "        self.discriminador = models.Sequential([\n",
        "            # Primeira camada: 512 neurônios processando dados de entrada\n",
        "            layers.Dense(512, activation='relu',\n",
        "                        input_shape=(self.dimensao_entrada,)),\n",
        "            layers.Dropout(0.3),  # Dropout para regularização - previte overfitting\n",
        "\n",
        "            # Segunda camada: 256 neurônios\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Terceira camada: 128 neurônios\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Camada de saída: 1 neurônio com sigmoid para probabilidade [0,1]\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        # Compila o discriminador separadamente\n",
        "        self.discriminador.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',  # Loss para classificação binária\n",
        "            metrics=['accuracy']         # Acompanha acurácia\n",
        "        )\n",
        "\n",
        "        print(f\"   Discriminador criado: {self.dimensao_entrada} → 512 → 256 → 128 → 1\")\n",
        "\n",
        "    def construir_gan(self):\n",
        "        \"\"\"Constrói a GAN completa conectando gerador ao discriminador\"\"\"\n",
        "        print(\" Conectando GAN completa...\")\n",
        "\n",
        "        # Congela o discriminador durante o treino do gerador\n",
        "        # Isso é CRUCIAL: quando treinamos o gerador, não queremos atualizar o discriminador\n",
        "        self.discriminador.trainable = False\n",
        "\n",
        "        # Conecta gerador -> discriminador\n",
        "        self.gan = models.Sequential([\n",
        "            self.gerador,      # Gera dados falsos a partir do ruído\n",
        "            self.discriminador # Classifica os dados gerados\n",
        "        ])\n",
        "\n",
        "        # Compila a GAN completa\n",
        "        self.gan.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy'  # Gerador tenta fazer discriminador prever \"1\" para dados falsos\n",
        "        )\n",
        "\n",
        "        print(\"   GAN completa construída e compilada!\")\n",
        "\n",
        "    def treinar(self, dados_reais, epocas=10000, batch_size=32, intervalo_exibicao=1000):\n",
        "        \"\"\"\n",
        "        Treina a GAN usando o processo adversarial\n",
        "\n",
        "        Args:\n",
        "            dados_reais (array): Dataset de dados reais para treinamento\n",
        "            epocas (int): Número total de épocas de treinamento\n",
        "            batch_size (int): Tamanho do batch para treinamento\n",
        "            intervalo_exibicao (int): Frequência para exibir progresso\n",
        "        \"\"\"\n",
        "        print(f\" Iniciando treinamento por {epocas} épocas...\")\n",
        "        print(f\"    Dados reais: {dados_reais.shape}\")\n",
        "        print(f\"    Batch size: {batch_size}\")\n",
        "\n",
        "        # Normaliza dados reais para [-1, 1] (compatível com tanh do gerador)\n",
        "        self.scaler = StandardScaler()\n",
        "        dados_reais_normalizados = self.scaler.fit_transform(dados_reais)\n",
        "\n",
        "        # Para visualização do progresso\n",
        "        self.fig, self.axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        self.fig.suptitle('Progresso do Treinamento da GAN', fontsize=16)\n",
        "\n",
        "        inicio_tempo = time.time()\n",
        "\n",
        "        for epoca in range(epocas):\n",
        "            # === FASE 1: TREINAR DISCRIMINADOR ===\n",
        "            # O discriminador aprende a distinguir dados reais de falsos\n",
        "\n",
        "            # Gera dados falsos usando o gerador\n",
        "            ruido = np.random.normal(0, 1, (batch_size, self.dimensao_latente))\n",
        "            dados_gerados = self.gerador.predict(ruido, verbose=0)\n",
        "\n",
        "            # Seleciona batch aleatório de dados reais\n",
        "            idx = np.random.randint(0, dados_reais_normalizados.shape[0], batch_size)\n",
        "            batch_reais = dados_reais_normalizados[idx]\n",
        "\n",
        "            # Rótulos:\n",
        "            # - Reais = 0.9 (label smoothing - ajuda a prevenir overfitting)\n",
        "            # - Falsos = 0.0\n",
        "            rotulos_reais = np.ones((batch_size, 1)) * 0.9  # Label smoothing\n",
        "            rotulos_falsos = np.zeros((batch_size, 1))\n",
        "\n",
        "            # Treina discriminador em duas etapas:\n",
        "            # 1. Com dados reais (aprende a prever ~1)\n",
        "            d_loss_real = self.discriminador.train_on_batch(\n",
        "                batch_reais, rotulos_reais)\n",
        "\n",
        "            # 2. Com dados falsos (aprende a prever ~0)\n",
        "            d_loss_fake = self.discriminador.train_on_batch(\n",
        "                dados_gerados, rotulos_falsos)\n",
        "\n",
        "            # Loss média do discriminador\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # === FASE 2: TREINAR GERADOR ===\n",
        "            # O gerador aprende a enganar o discriminador\n",
        "\n",
        "            # Gera novo ruído\n",
        "            ruido = np.random.normal(0, 1, (batch_size, self.dimensao_latente))\n",
        "\n",
        "            # Rótulos enganosos: faz o discriminador pensar que dados falsos são reais\n",
        "            rotulos_enganosos = np.ones((batch_size, 1))\n",
        "\n",
        "            # Treina o gerador (através da GAN completa)\n",
        "            # Aqui só o gerador é atualizado (discriminador está congelado)\n",
        "            g_loss = self.gan.train_on_batch(ruido, rotulos_enganosos)\n",
        "\n",
        "            # Armazena histórico para análise\n",
        "            self.historico_loss_d.append(d_loss[0])\n",
        "            self.historico_loss_g.append(g_loss)\n",
        "            self.historico_acc_d.append(d_loss[1])\n",
        "\n",
        "            # Exibe progresso e visualizações\n",
        "            if epoca % intervalo_exibicao == 0:\n",
        "                tempo_decorrido = time.time() - inicio_tempo\n",
        "                print(f\"⏱️  Época {epoca}/{epocas} | \"\n",
        "                      f\"D Loss: {d_loss[0]:.4f} | \"\n",
        "                      f\"G Loss: {g_loss:.4f} | \"\n",
        "                      f\"D Acc: {d_loss[1]:.2f} | \"\n",
        "                      f\"Tempo: {tempo_decorrido:.1f}s\")\n",
        "\n",
        "                self.visualizar_progresso(epoca, dados_reais_normalizados)\n",
        "\n",
        "        print(f\" Treinamento concluído em {time.time() - inicio_tempo:.1f} segundos!\")\n",
        "\n",
        "    def visualizar_progresso(self, epoca, dados_reais):\n",
        "        \"\"\"Visualiza o progresso do treinamento\"\"\"\n",
        "        # Gera dados de exemplo para visualização\n",
        "        ruido_exemplo = np.random.normal(0, 1, (1000, self.dimensao_latente))\n",
        "        dados_gerados = self.gerador.predict(ruido_exemplo, verbose=0)\n",
        "\n",
        "        # Limpa os axes para nova plotagem\n",
        "        for ax in self.axes.flat:\n",
        "            ax.clear()\n",
        "\n",
        "        # Plot 1: Dados reais vs gerados\n",
        "        self.axes[0, 0].scatter(dados_reais[:, 0], dados_reais[:, 1],\n",
        "                               alpha=0.5, label='Reais', s=10)\n",
        "        self.axes[0, 0].scatter(dados_gerados[:, 0], dados_gerados[:, 1],\n",
        "                               alpha=0.5, label='Gerados', s=10)\n",
        "        self.axes[0, 0].set_title(f'Época {epoca}: Dados Reais vs Gerados')\n",
        "        self.axes[0, 0].legend()\n",
        "        self.axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Evolução das losses\n",
        "        self.axes[0, 1].plot(self.historico_loss_d, label='Discriminador')\n",
        "        self.axes[0, 1].plot(self.historico_loss_g, label='Gerador')\n",
        "        self.axes[0, 1].set_title('Evolução das Losses')\n",
        "        self.axes[0, 1].set_xlabel('Época')\n",
        "        self.axes[0, 1].set_ylabel('Loss')\n",
        "        self.axes[0, 1].legend()\n",
        "        self.axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Acurácia do discriminador\n",
        "        self.axes[0, 2].plot(self.historico_acc_d)\n",
        "        self.axes[0, 2].set_title('Acurácia do Discriminador')\n",
        "        self.axes[0, 2].set_xlabel('Época')\n",
        "        self.axes[0, 2].set_ylabel('Acurácia')\n",
        "        self.axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Distribuição dos dados reais (feature 1)\n",
        "        self.axes[1, 0].hist(dados_reais[:, 0], bins=30, alpha=0.7, label='Reais', density=True)\n",
        "        self.axes[1, 0].hist(dados_gerados[:, 0], bins=30, alpha=0.7, label='Gerados', density=True)\n",
        "        self.axes[1, 0].set_title('Distribuição - Feature 1')\n",
        "        self.axes[1, 0].legend()\n",
        "\n",
        "        # Plot 5: Distribuição dos dados reais (feature 2)\n",
        "        self.axes[1, 1].hist(dados_reais[:, 1], bins=30, alpha=0.7, label='Reais', density=True)\n",
        "        self.axes[1, 1].hist(dados_gerados[:, 1], bins=30, alpha=0.7, label='Gerados', density=True)\n",
        "        self.axes[1, 1].set_title('Distribuição - Feature 2')\n",
        "        self.axes[1, 1].legend()\n",
        "\n",
        "        # Plot 6: Espaço latente para geração\n",
        "        ruido_visual = np.random.normal(0, 1, (100, self.dimensao_latente))\n",
        "        dados_visual = self.gerador.predict(ruido_visual, verbose=0)\n",
        "        self.axes[1, 2].scatter(dados_visual[:, 0], dados_visual[:, 1], alpha=0.6)\n",
        "        self.axes[1, 2].set_title('Amostras Geradas Recentes')\n",
        "        self.axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.pause(0.1)\n",
        "        plt.draw()\n",
        "\n",
        "    def gerar_dados(self, n_amostras=1000):\n",
        "        \"\"\"Gera novos dados sintéticos usando o gerador treinado\"\"\"\n",
        "        ruido = np.random.normal(0, 1, (n_amostras, self.dimensao_latente))\n",
        "        dados_gerados = self.gerador.predict(ruido, verbose=0)\n",
        "\n",
        "        # Se usamos scaler, revertemos a normalização\n",
        "        if hasattr(self, 'scaler'):\n",
        "            dados_gerados = self.scaler.inverse_transform(dados_gerados)\n",
        "\n",
        "        return dados_gerados\n",
        "\n",
        "    def avaliar_gerador(self, dados_reais, n_amostras=1000):\n",
        "        \"\"\"Avalia a qualidade dos dados gerados\"\"\"\n",
        "        dados_gerados = self.gerar_dados(n_amostras)\n",
        "\n",
        "        # Métricas simples de avaliação\n",
        "        stats_reais = {\n",
        "            'mean': np.mean(dados_reais, axis=0),\n",
        "            'std': np.std(dados_reais, axis=0),\n",
        "            'min': np.min(dados_reais, axis=0),\n",
        "            'max': np.max(dados_reais, axis=0)\n",
        "        }\n",
        "\n",
        "        stats_gerados = {\n",
        "            'mean': np.mean(dados_gerados, axis=0),\n",
        "            'std': np.std(dados_gerados, axis=0),\n",
        "            'min': np.min(dados_gerados, axis=0),\n",
        "            'max': np.max(dados_gerados, axis=0)\n",
        "        }\n",
        "\n",
        "        print(\"\\n ESTATÍSTICAS COMPARATIVAS:\")\n",
        "        print(\"   Dados Reais vs Dados Gerados\")\n",
        "        for stat in ['mean', 'std', 'min', 'max']:\n",
        "            print(f\"   {stat.upper():4}: \"\n",
        "                  f\"Reais {stats_reais[stat]} | \"\n",
        "                  f\"Gerados {stats_gerados[stat]}\")\n",
        "\n",
        "        return dados_gerados\n",
        "\n",
        "# EXEMPLO 1: GAN para Dataset \"Two Moons\"\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 1: GAN PARA DATASET 'TWO MOONS'\")\n",
        "\n",
        "\n",
        "# Cria dataset de duas luas (2D - fácil de visualizar)\n",
        "X_moons, y_moons = make_moons(n_samples=2000, noise=0.1, random_state=42)\n",
        "print(f\" Dataset Two Moons: {X_moons.shape}\")\n",
        "\n",
        "# Visualiza dados reais\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis', alpha=0.6)\n",
        "plt.title('Dados Reais - Two Moons')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Cria e treina GAN\n",
        "gan_moons = GANSimples(\n",
        "    dimensao_latente=10,    # Espaço latente de 10 dimensões\n",
        "    dimensao_entrada=2      # Dados 2D\n",
        ")\n",
        "\n",
        "# Treina por um número menor de épocas para demonstração\n",
        "gan_moons.treinar(\n",
        "    dados_reais=X_moons,\n",
        "    epocas=5000,           # Número reduzido para demonstração\n",
        "    batch_size=64,\n",
        "    intervalo_exibicao=500\n",
        ")\n",
        "\n",
        "# Gera e avalia dados sintéticos\n",
        "dados_gerados_moons = gan_moons.avaliar_gerador(X_moons)\n",
        "\n",
        "# Visualização final\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], alpha=0.3, label='Reais', s=20)\n",
        "plt.scatter(dados_gerados_moons[:, 0], dados_gerados_moons[:, 1],\n",
        "           alpha=0.3, label='Gerados', s=20)\n",
        "plt.title('Comparação Final: Reais vs Gerados')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Mostra a evolução das losses\n",
        "epocas = range(len(gan_moons.historico_loss_d))\n",
        "plt.plot(epocas, gan_moons.historico_loss_d, label='Loss Discriminador')\n",
        "plt.plot(epocas, gan_moons.historico_loss_g, label='Loss Gerador')\n",
        "plt.title('Evolução das Losses')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 2: GAN para Dataset Circular\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 2: GAN PARA DATASET CIRCULAR\")\n",
        "\n",
        "\n",
        "# Cria dataset circular (mais desafiador)\n",
        "X_circles, y_circles = make_circles(n_samples=2000, noise=0.05, factor=0.5, random_state=42)\n",
        "print(f\" Dataset Circular: {X_circles.shape}\")\n",
        "\n",
        "# Cria GAN com arquitetura diferente\n",
        "gan_circles = GANSimples(\n",
        "    dimensao_latente=20,    # Espaço latente maior para padrão mais complexo\n",
        "    dimensao_entrada=2\n",
        ")\n",
        "\n",
        "# Treina a GAN\n",
        "gan_circles.treinar(\n",
        "    dados_reais=X_circles,\n",
        "    epocas=8000,\n",
        "    batch_size=128,\n",
        "    intervalo_exibicao=1000\n",
        ")\n",
        "\n",
        "# Gera dados e avalia\n",
        "dados_gerados_circles = gan_circles.avaliar_gerador(X_circles)\n",
        "\n",
        "# Visualização comparativa\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='viridis', alpha=0.6)\n",
        "plt.title('Dados Reais - Círculos')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_circles[:, 0], X_circles[:, 1], alpha=0.2, label='Reais', s=20)\n",
        "plt.scatter(dados_gerados_circles[:, 0], dados_gerados_circles[:, 1],\n",
        "           alpha=0.5, label='Gerados', s=20)\n",
        "plt.title('Dados Gerados pela GAN')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLO 3: GAN para Dados Multidimensionais\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 3: GAN PARA DADOS MULTIDIMENSIONAIS\")\n",
        "\n",
        "\n",
        "# Cria dataset multidimensional sintético\n",
        "np.random.seed(42)\n",
        "n_features = 5\n",
        "n_samples = 3000\n",
        "\n",
        "# Cria dados com correlações complexas\n",
        "X_multi = np.random.normal(0, 1, (n_samples, n_features))\n",
        "# Adiciona correlações não-lineares\n",
        "X_multi[:, 2] = X_multi[:, 0] ** 2 + 0.1 * np.random.normal(0, 1, n_samples)\n",
        "X_multi[:, 3] = np.sin(X_multi[:, 1]) + 0.1 * np.random.normal(0, 1, n_samples)\n",
        "X_multi[:, 4] = X_multi[:, 0] * X_multi[:, 1] + 0.1 * np.random.normal(0, 1, n_samples)\n",
        "\n",
        "print(f\" Dataset Multidimensional: {X_multi.shape}\")\n",
        "\n",
        "# Cria GAN para dados multidimensionais\n",
        "gan_multi = GANSimples(\n",
        "    dimensao_latente=20,\n",
        "    dimensao_entrada=n_features\n",
        ")\n",
        "\n",
        "# Treina a GAN\n",
        "gan_multi.treinar(\n",
        "    dados_reais=X_multi,\n",
        "    epocas=10000,\n",
        "    batch_size=128,\n",
        "    intervalo_exibicao=2000\n",
        ")\n",
        "\n",
        "# Gera e avalia dados sintéticos\n",
        "dados_gerados_multi = gan_multi.avaliar_gerador(X_multi)\n",
        "\n",
        "# Visualiza correlações\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Pares de features para visualizar correlações\n",
        "pares = [(0, 1), (0, 2), (1, 3), (2, 4), (3, 4)]\n",
        "\n",
        "for idx, (i, j) in enumerate(pares):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    ax.scatter(X_multi[:, i], X_multi[:, j], alpha=0.3, label='Reais', s=20)\n",
        "    ax.scatter(dados_gerados_multi[:, i], dados_gerados_multi[:, j],\n",
        "              alpha=0.3, label='Gerados', s=20)\n",
        "    ax.set_title(f'Feature {i} vs Feature {j}')\n",
        "    ax.set_xlabel(f'Feature {i}')\n",
        "    ax.set_ylabel(f'Feature {j}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Último subplot para estatísticas\n",
        "ax = axes[1, 2]\n",
        "features = range(n_features)\n",
        "means_reais = np.mean(X_multi, axis=0)\n",
        "means_gerados = np.mean(dados_gerados_multi, axis=0)\n",
        "\n",
        "ax.bar(np.array(features) - 0.2, means_reais, 0.4, label='Reais', alpha=0.7)\n",
        "ax.bar(np.array(features) + 0.2, means_gerados, 0.4, label='Gerados', alpha=0.7)\n",
        "ax.set_title('Médias das Features')\n",
        "ax.set_xlabel('Feature')\n",
        "ax.set_ylabel('Média')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# DICAS E MELHORES PRÁTICAS\n",
        "\n",
        "\n",
        "\n",
        "print(\" DICAS E MELHORES PRÁTICAS PARA GANs\")\n",
        "\n",
        "\n",
        "dicas = {\n",
        "    \"Balanceamento\": \"Loss do discriminador ~0.5 indica bom balanceamento\",\n",
        "    \"Collapse\": \"Se G loss → 0 e D loss → 0, pode haver mode collapse\",\n",
        "    \"Arquitetura\": \"Gerador geralmente mais complexo que discriminador\",\n",
        "    \"Normalização\": \"BatchNorm no gerador, Dropout no discriminador\",\n",
        "    \"Learning Rate\": \"Taxas de aprendizado menores (1e-4 a 1e-5) funcionam melhor\",\n",
        "    \"Label Smoothing\": \"Usar 0.9/0.1 em vez de 1/0 previne overfitting do discriminador\"\n",
        "}\n",
        "\n",
        "for dica, explicacao in dicas.items():\n",
        "    print(f\"    {dica}: {explicacao}\")\n",
        "\n",
        "print(f\"\\n PROBLEMAS COMUNS E SOLUÇÕES:\")\n",
        "problemas = [\n",
        "    \"Mode Collapse: Gerador produz pouca variedade → Aumentar diversidade do ruído\",\n",
        "    \"Discriminador Muito Forte: G não aprende → Reduzir capacidade do D\",\n",
        "    \"Instabilidade: Losses oscilam muito → Reduzir learning rate\",\n",
        "    \"Vanishing Gradients: D muito bom → Usar Wasserstein GAN ou label smoothing\"\n",
        "]\n",
        "\n",
        "for i, problema in enumerate(problemas, 1):\n",
        "    print(f\"   {i}. {problema}\")\n",
        "\n",
        "print(f\"\\n MÉTRICAS DE SUCESSO:\")\n",
        "metricas = [\n",
        "    \" Discriminador tem acurácia ~50% (não consegue distinguir)\",\n",
        "    \" Dados gerados têm estatísticas similares aos reais\",\n",
        "    \" Visualmente, dados gerados são indistinguíveis dos reais\",\n",
        "    \"Losses estabilizam em valores balanceados\"\n",
        "]\n",
        "\n",
        "for metrica in metricas:\n",
        "    print(f\"   {metrica}\")\n",
        "\n",
        "\n",
        "# Exemplo de uso prático para gerar mais dados\n",
        "\n",
        "print(\" GERANDO NOVOS DADOS SINTÉTICOS\")\n",
        "\n",
        "\n",
        "# Gera 1000 novas amostras sintéticas\n",
        "novos_dados = gan_moons.gerar_dados(1000)\n",
        "print(f\"Gerados {len(novos_dados)} novas amostras sintéticas\")\n",
        "\n",
        "# Podemos usar esses dados para aumentar datasets de treinamento\n",
        "# ou para testes onde precisamos de mais variedade de dados\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_moons[:, 0], X_moons[:, 1], alpha=0.3, label='Originais', s=20)\n",
        "plt.title('Dataset Original')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(novos_dados[:, 0], novos_dados[:, 1], alpha=0.3, label='Sintéticos', s=20)\n",
        "plt.title('Dados Sintéticos Gerados')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j4Cai3jCu52V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Conceitos Fundamentais"
      ],
      "metadata": {
        "id": "6dADaZjLu7-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1 Camadas Ocultas e Neurônios\n",
        "\n",
        "O que são: Elementos internos da rede que processam informações.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Camadas Ocultas:\n",
        "\n",
        "        São as camadas entre a entrada e saída\n",
        "\n",
        "        Extraem características hierárquicas dos dados\n",
        "\n",
        "        Cada camada aprende representações mais abstratas\n",
        "\n",
        "    Neurônios:\n",
        "\n",
        "        Unidades de processamento individuais\n",
        "\n",
        "        Aplicam funções de ativação nas entradas ponderadas\n",
        "\n",
        "    Como escolher:\n",
        "\n",
        "        Regra dos 2/3: (entradas + saídas) × 2/3\n",
        "\n",
        "        Baseado em dados: n_amostras / (α × (entradas + saídas))\n",
        "\n",
        "        Experimentation: Testar diferentes arquiteturas"
      ],
      "metadata": {
        "id": "w6e3SZTGu_p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "class AnaliseArquitetura:\n",
        "    \"\"\"Classe completa para análise e sugestão de arquiteturas de redes neurais\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calcular_neurônios_total(arquitetura):\n",
        "        \"\"\"\n",
        "        Calcula o total de neurônios nas camadas ocultas\n",
        "\n",
        "        Args:\n",
        "            arquitetura (list): Lista com número de neurônios por camada\n",
        "                Ex: [10, 64, 32, 1] = 10 entradas, 64+32 ocultos, 1 saída\n",
        "\n",
        "        Returns:\n",
        "            int: Total de neurônios nas camadas ocultas\n",
        "        \"\"\"\n",
        "        # Soma todas as camadas exceto a primeira (entrada) e última (saída)\n",
        "        total_ocultos = sum(arquitetura[1:-1])\n",
        "\n",
        "        print(f\" Arquitetura: {arquitetura}\")\n",
        "        print(f\"   → Camadas ocultas: {arquitetura[1:-1]}\")\n",
        "        print(f\"   → Total neurônios ocultos: {total_ocultos}\")\n",
        "\n",
        "        return total_ocultos\n",
        "\n",
        "    @staticmethod\n",
        "    def sugerir_arquitetura(dimensao_entrada, dimensao_saida, complexidade_problema='medio', n_amostras=None):\n",
        "        \"\"\"\n",
        "        Sugere arquitetura baseada na complexidade do problema e número de amostras\n",
        "\n",
        "        Args:\n",
        "            dimensao_entrada (int): Número de features de entrada\n",
        "            dimensao_saida (int): Número de neurônios de saída\n",
        "            complexidade_problema (str): 'simples', 'medio', 'complexo'\n",
        "            n_amostras (int): Número de amostras no dataset (opcional)\n",
        "\n",
        "        Returns:\n",
        "            list: Arquitetura sugerida\n",
        "        \"\"\"\n",
        "        print(f\"Analisando problema: {dimensao_entrada}D → {dimensao_saida}D\")\n",
        "        print(f\"   Complexidade: {complexidade_problema}\")\n",
        "\n",
        "        # Arquiteturas base baseadas na complexidade\n",
        "        sugestoes_base = {\n",
        "            'simples': [dimensao_entrada, dimensao_saida],  # Sem camadas ocultas\n",
        "            'medio': [dimensao_entrada, 64, 32, dimensao_saida],  # 2 camadas ocultas\n",
        "            'complexo': [dimensao_entrada, 256, 128, 64, 32, dimensao_saida]  # 4 camadas ocultas\n",
        "        }\n",
        "\n",
        "        arquitetura = sugestoes_base.get(complexidade_problema, sugestoes_base['medio'])\n",
        "\n",
        "        # Ajusta baseado no número de amostras se fornecido\n",
        "        if n_amostras:\n",
        "            arquitetura = AnaliseArquitetura.ajustar_por_amostras(\n",
        "                arquitetura, n_amostras, dimensao_entrada, dimensao_saida\n",
        "            )\n",
        "\n",
        "        print(f\"    Arquitetura sugerida: {arquitetura}\")\n",
        "        AnaliseArquitetura.calcular_neurônios_total(arquitetura)\n",
        "\n",
        "        return arquitetura\n",
        "\n",
        "    @staticmethod\n",
        "    def regra_thumb_neurônios(dimensao_entrada, dimensao_saida, n_amostras):\n",
        "        \"\"\"\n",
        "        Regras práticas para determinar número de neurônios em camada oculta\n",
        "\n",
        "        Args:\n",
        "            dimensao_entrada (int): Número de features de entrada\n",
        "            dimensao_saida (int): Número de neurônios de saída\n",
        "            n_amostras (int): Número de amostras no dataset\n",
        "\n",
        "        Returns:\n",
        "            int: Número sugerido de neurônios para camada oculta\n",
        "        \"\"\"\n",
        "        print(f\" Aplicando regras práticas:\")\n",
        "        print(f\"   Entrada: {dimensao_entrada}, Saída: {dimensao_saida}, Amostras: {n_amostras}\")\n",
        "\n",
        "        # REGRA 1: Média entre entrada e saída (2/3)\n",
        "        neurônios_intermediarios = int((dimensao_entrada + dimensao_saida) * 2/3)\n",
        "        print(f\"    Regra dos 2/3: ({dimensao_entrada} + {dimensao_saida}) × 2/3 = {neurônios_intermediarios}\")\n",
        "\n",
        "        # REGRA 2: Baseado no número de amostras\n",
        "        neurônios_amostras = int(n_amostras / (5 * (dimensao_entrada + dimensao_saida)))\n",
        "        print(f\"    Baseado em amostras: {n_amostras} / (5 × ({dimensao_entrada} + {dimensao_saida})) = {neurônios_amostras}\")\n",
        "\n",
        "        # REGRA 3: Não exceder o número de amostras\n",
        "        limite_amostras = min(neurônios_intermediarios, neurônios_amostras, 1000)\n",
        "\n",
        "        # REGRA 4: Mínimo prático\n",
        "        neurônios_final = max(limite_amostras, 16)  # Mínimo de 16 neurônios\n",
        "\n",
        "        print(f\"    Limite por amostras: {limite_amostras}\")\n",
        "        print(f\"    Valor final (com mínimo 16): {neurônios_final}\")\n",
        "\n",
        "        return neurônios_final\n",
        "\n",
        "    @staticmethod\n",
        "    def ajustar_por_amostras(arquitetura, n_amostras, dimensao_entrada, dimensao_saida):\n",
        "        \"\"\"\n",
        "        Ajusta arquitetura baseado no número de amostras disponíveis\n",
        "        \"\"\"\n",
        "        print(f\"    Ajustando por {n_amostras} amostras...\")\n",
        "\n",
        "        # Calcula neurônios sugeridos pela regra prática\n",
        "        neurônios_sugeridos = AnaliseArquitetura.regra_thumb_neurônios(\n",
        "            dimensao_entrada, dimensao_saida, n_amostras\n",
        "        )\n",
        "\n",
        "        # Ajusta camadas ocultas baseado na sugestão\n",
        "        nova_arquitetura = [arquitetura[0]]  # Mantém entrada\n",
        "\n",
        "        # Para arquiteturas com múltiplas camadas ocultas\n",
        "        if len(arquitetura) > 2:\n",
        "            # Reduz progressivamente da sugestão até a saída\n",
        "            camadas_ocultas = len(arquitetura) - 2\n",
        "            for i in range(camadas_ocultas):\n",
        "                # Redução progressiva: 100% → 50% → 25% etc\n",
        "                fator = 2 ** (i + 1)\n",
        "                neurônios_camada = max(neurônios_sugeridos // fator, 8)  # Mínimo 8\n",
        "                nova_arquitetura.append(neurônios_camada)\n",
        "        else:\n",
        "            # Se não tem camadas ocultas, adiciona uma\n",
        "            nova_arquitetura.append(neurônios_sugeridos)\n",
        "\n",
        "        nova_arquitetura.append(arquitetura[-1])  # Mantém saída\n",
        "\n",
        "        print(f\"    Arquitetura ajustada: {nova_arquitetura}\")\n",
        "        return nova_arquitetura\n",
        "\n",
        "    @staticmethod\n",
        "    def calcular_parametros(arquitetura):\n",
        "        \"\"\"\n",
        "        Calcula número total de parâmetros (pesos + vieses) da rede\n",
        "\n",
        "        Args:\n",
        "            arquitetura (list): Lista com número de neurônios por camada\n",
        "\n",
        "        Returns:\n",
        "            int: Número total de parâmetros treináveis\n",
        "        \"\"\"\n",
        "        total_parametros = 0\n",
        "        detalhes = []\n",
        "\n",
        "        for i in range(len(arquitetura) - 1):\n",
        "            # Pesos: neurônios_atual × neurônios_proxima\n",
        "            pesos = arquitetura[i] * arquitetura[i + 1]\n",
        "            # Vieses: neurônios_proxima_camada\n",
        "            vieses = arquitetura[i + 1]\n",
        "            # Total na camada\n",
        "            parametros_camada = pesos + vieses\n",
        "            total_parametros += parametros_camada\n",
        "\n",
        "            detalhes.append(f\"Camada {i}→{i+1}: {arquitetura[i]}×{arquitetura[i+1]} = {pesos} pesos + {vieses} vieses = {parametros_camada} parâmetros\")\n",
        "\n",
        "        print(f\" CÁLCULO DE PARÂMETROS:\")\n",
        "        for detalhe in detalhes:\n",
        "            print(f\"   {detalhe}\")\n",
        "        print(f\"    TOTAL: {total_parametros} parâmetros treináveis\")\n",
        "\n",
        "        return total_parametros\n",
        "\n",
        "    @staticmethod\n",
        "    def analisar_overfitting(n_amostras, n_parametros, limite_seguro=10):\n",
        "        \"\"\"\n",
        "        Analisa risco de overfitting baseado na relação amostras/parâmetros\n",
        "\n",
        "        Args:\n",
        "            n_amostras (int): Número de amostras de treinamento\n",
        "            n_parametros (int): Número de parâmetros do modelo\n",
        "            limite_seguro (int): Limite seguro de amostras por parâmetro\n",
        "\n",
        "        Returns:\n",
        "            dict: Análise de risco\n",
        "        \"\"\"\n",
        "        razao = n_amostras / n_parametros if n_parametros > 0 else float('inf')\n",
        "\n",
        "        print(f\"ANÁLISE DE OVERFITTING:\")\n",
        "        print(f\"   Amostras: {n_amostras}\")\n",
        "        print(f\"   Parâmetros: {n_parametros}\")\n",
        "        print(f\"   Razão amostras/parâmetros: {razao:.2f}:1\")\n",
        "\n",
        "        if razao >= limite_seguro:\n",
        "            risco = \"BAIXO\"\n",
        "            recomendacao = \"Arquitetura adequada\"\n",
        "        elif razao >= limite_seguro / 2:\n",
        "            risco = \"MODERADO\"\n",
        "            recomendacao = \"Considere regularização\"\n",
        "        else:\n",
        "            risco = \"ALTO\"\n",
        "            recomendacao = \"Reduza arquitetura ou aumente dados\"\n",
        "\n",
        "        print(f\"    Risco de overfitting: {risco}\")\n",
        "        print(f\"    Recomendação: {recomendacao}\")\n",
        "\n",
        "        return {\n",
        "            'razao': razao,\n",
        "            'risco': risco,\n",
        "            'recomendacao': recomendacao\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def comparar_arquiteturas(dimensao_entrada, dimensao_saida, n_amostras):\n",
        "        \"\"\"\n",
        "        Compara diferentes arquiteturas para o mesmo problema\n",
        "        \"\"\"\n",
        "        print(\" COMPARANDO ARQUITETURAS:\")\n",
        "\n",
        "        arquiteturas = {\n",
        "            'Simples': [dimensao_entrada, dimensao_saida],\n",
        "            'Médio': [dimensao_entrada, 64, 32, dimensao_saida],\n",
        "            'Complexo': [dimensao_entrada, 256, 128, 64, 32, dimensao_saida],\n",
        "            'Ajustado': AnaliseArquitetura.sugerir_arquitetura(\n",
        "                dimensao_entrada, dimensao_saida, 'medio', n_amostras\n",
        "            )\n",
        "        }\n",
        "\n",
        "        resultados = []\n",
        "\n",
        "        for nome, arquitetura in arquiteturas.items():\n",
        "            print(f\"\\n {nome}: {arquitetura}\")\n",
        "            neurônios = AnaliseArquitetura.calcular_neurônios_total(arquitetura)\n",
        "            parametros = AnaliseArquitetura.calcular_parametros(arquitetura)\n",
        "            analise = AnaliseArquitetura.analisar_overfitting(n_amostras, parametros)\n",
        "\n",
        "            resultados.append({\n",
        "                'nome': nome,\n",
        "                'arquitetura': arquitetura,\n",
        "                'neurônios_ocultos': neurônios,\n",
        "                'parametros': parametros,\n",
        "                'risco': analise['risco']\n",
        "            })\n",
        "\n",
        "        # Visualização comparativa\n",
        "        AnaliseArquitetura.plotar_comparacao(resultados, n_amostras)\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    @staticmethod\n",
        "    def plotar_comparacao(resultados, n_amostras):\n",
        "        \"\"\"Plota comparação visual entre arquiteturas\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle(f'Comparação de Arquiteturas (Base: {n_amostras} amostras)', fontsize=16)\n",
        "\n",
        "        # Dados para plots\n",
        "        nomes = [r['nome'] for r in resultados]\n",
        "        neurônios = [r['neurônios_ocultos'] for r in resultados]\n",
        "        parametros = [r['parametros'] for r in resultados]\n",
        "        riscos = [r['risco'] for r in resultados]\n",
        "\n",
        "        # Mapeamento de cores para risco\n",
        "        cores_risco = {'BAIXO': 'green', 'MODERADO': 'orange', 'ALTO': 'red'}\n",
        "        cores = [cores_risco[risco] for risco in riscos]\n",
        "\n",
        "        # Plot 1: Neurônios ocultos\n",
        "        bars1 = axes[0, 0].bar(nomes, neurônios, color=cores, alpha=0.7)\n",
        "        axes[0, 0].set_title('Total de Neurônios Ocultos')\n",
        "        axes[0, 0].set_ylabel('Neurônios')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Adiciona valores nas barras\n",
        "        for bar, valor in zip(bars1, neurônios):\n",
        "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{valor}', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 2: Parâmetros totais\n",
        "        bars2 = axes[0, 1].bar(nomes, parametros, color=cores, alpha=0.7)\n",
        "        axes[0, 1].set_title('Total de Parâmetros Treináveis')\n",
        "        axes[0, 1].set_ylabel('Parâmetros')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        for bar, valor in zip(bars2, parametros):\n",
        "            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{valor:,}', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 3: Razão amostras/parâmetros\n",
        "        razoes = [n_amostras / p if p > 0 else 0 for p in parametros]\n",
        "        bars3 = axes[1, 0].bar(nomes, razoes, color=cores, alpha=0.7)\n",
        "        axes[1, 0].set_title('Razão Amostras/Parâmetros')\n",
        "        axes[1, 0].set_ylabel('Razão')\n",
        "        axes[1, 0].axhline(y=10, color='red', linestyle='--', label='Limite seguro (10:1)')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        for bar, valor in zip(bars3, razoes):\n",
        "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{valor:.1f}:1', ha='center', va='bottom')\n",
        "\n",
        "        # Plot 4: Legenda de riscos\n",
        "        axes[1, 1].axis('off')\n",
        "        info_text = \"LEGENDA DE RISCO:\\n\\n\"\n",
        "        for risco, cor in cores_risco.items():\n",
        "            info_text += f\"● {risco}: {cor.upper()}\\n\"\n",
        "\n",
        "        info_text += f\"\\nRECOMENDAÇÕES:\\n\"\n",
        "        info_text += f\"• >10:1 → Baixo risco\\n\"\n",
        "        info_text += f\"• 5-10:1 → Moderado\\n\"\n",
        "        info_text += f\"• <5:1 → Alto risco\"\n",
        "\n",
        "        axes[1, 1].text(0.1, 0.9, info_text, fontsize=12, va='top', linespacing=1.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# EXEMPLOS PRÁTICOS DE USO\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 1: PROBLEMA DE CLASSIFICAÇÃO SIMPLES\")\n",
        "\n",
        "\n",
        "# Problema: 10 features → classificação binária\n",
        "dimensao_entrada = 10\n",
        "dimensao_saida = 1\n",
        "n_amostras = 1000\n",
        "\n",
        "print(\" PROBLEMA: Classificação binária com 10 features\")\n",
        "print(f\"   Entrada: {dimensao_entrada}D, Saída: {dimensao_saida}D, Amostras: {n_amostras}\")\n",
        "\n",
        "# 1. Sugerir arquitetura\n",
        "arquitetura_sugerida = AnaliseArquitetura.sugerir_arquitetura(\n",
        "    dimensao_entrada, dimensao_saida, 'medio', n_amostras\n",
        ")\n",
        "\n",
        "# 2. Calcular parâmetros\n",
        "parametros = AnaliseArquitetura.calcular_parametros(arquitetura_sugerida)\n",
        "\n",
        "# 3. Analisar risco de overfitting\n",
        "analise = AnaliseArquitetura.analisar_overfitting(n_amostras, parametros)\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 2: PROBLEMA COMPLEXO - VISÃO COMPUTACIONAL\")\n",
        "\n",
        "\n",
        "# Problema: Imagens 32x32 RGB → 10 classes\n",
        "dimensao_entrada = 32 * 32 * 3  # 3072 pixels\n",
        "dimensao_saida = 10\n",
        "n_amostras = 50000\n",
        "\n",
        "print(\" PROBLEMA: Classificação de imagens 32x32 RGB em 10 classes\")\n",
        "print(f\"   Entrada: {dimensao_entrada}D, Saída: {dimensao_saida}D, Amostras: {n_amostras}\")\n",
        "\n",
        "# Usando regra prática para determinar neurônios\n",
        "neurônios_sugeridos = AnaliseArquitetura.regra_thumb_neurônios(\n",
        "    dimensao_entrada, dimensao_saida, n_amostras\n",
        ")\n",
        "\n",
        "# Arquitetura para problema complexo\n",
        "arquitetura_complexa = [\n",
        "    dimensao_entrada,\n",
        "    neurônios_sugeridos,\n",
        "    neurônios_sugeridos // 2,\n",
        "    neurônios_sugeridos // 4,\n",
        "    dimensao_saida\n",
        "]\n",
        "\n",
        "print(f\"    Arquitetura complexa sugerida: {arquitetura_complexa}\")\n",
        "\n",
        "# Análise completa\n",
        "parametros_complexo = AnaliseArquitetura.calcular_parametros(arquitetura_complexa)\n",
        "AnaliseArquitetura.analisar_overfitting(n_amostras, parametros_complexo)\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 3: COMPARAÇÃO DE MÚLTIPLAS ARQUITETURAS\")\n",
        "\n",
        "\n",
        "# Comparação para problema médio\n",
        "dimensao_entrada = 20\n",
        "dimensao_saida = 3  # 3 classes\n",
        "n_amostras = 2000\n",
        "\n",
        "print(\" PROBLEMA: Classificação multiclasse com 20 features\")\n",
        "print(f\"   Entrada: {dimensao_entrada}D, Saída: {dimensao_saida}D, Amostras: {n_amostras}\")\n",
        "\n",
        "# Compara diferentes abordagens\n",
        "resultados_comparacao = AnaliseArquitetura.comparar_arquiteturas(\n",
        "    dimensao_entrada, dimensao_saida, n_amostras\n",
        ")\n",
        "\n",
        "\n",
        "print(\" RESUMO DAS REGRAS PRÁTICAS\")\n",
        "\n",
        "\n",
        "regras = {\n",
        "    \"Regra dos 2/3\": \"(entrada + saída) × 2/3\",\n",
        "    \"Regra das Amostras\": \"amostras / (5 × (entrada + saída))\",\n",
        "    \"Mínimo Prático\": \"Pelo menos 16 neurônios\",\n",
        "    \"Máximo Seguro\": \"Máximo 1000 neurônios por camada\",\n",
        "    \"Razão Segura\": \"Pelo menos 10 amostras por parâmetro\",\n",
        "    \"Redução Progressiva\": \"Camadas seguintes com ~50% dos neurônios anteriores\"\n",
        "}\n",
        "\n",
        "for regra, formula in regras.items():\n",
        "    print(f\"    {regra}: {formula}\")\n",
        "\n",
        "\n",
        "# EXEMPLO 4: CRIAÇÃO DE MODELO COM ARQUITETURA SUGERIDA\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 4: IMPLEMENTANDO MODELO COM ARQUITETURA OTIMIZADA\")\n",
        "\n",
        "\n",
        "def criar_modelo_otimizado(dimensao_entrada, dimensao_saida, n_amostras, tipo_problema='classificacao'):\n",
        "    \"\"\"\n",
        "    Cria modelo Keras com arquitetura otimizada baseada na análise\n",
        "    \"\"\"\n",
        "    print(f\" CRIANDO MODELO OTIMIZADO:\")\n",
        "    print(f\"   Entrada: {dimensao_entrada}, Saída: {dimensao_saida}, Amostras: {n_amostras}\")\n",
        "\n",
        "    # Obtém arquitetura sugerida\n",
        "    arquitetura = AnaliseArquitetura.sugerir_arquitetura(\n",
        "        dimensao_entrada, dimensao_saida, 'medio', n_amostras\n",
        "    )\n",
        "\n",
        "    # Cria modelo sequencial\n",
        "    modelo = tf.keras.Sequential()\n",
        "\n",
        "    # Adiciona camadas baseado na arquitetura sugerida\n",
        "    for i in range(1, len(arquitetura)):\n",
        "        if i == 1:\n",
        "            # Primeira camada precisa de input_shape\n",
        "            modelo.add(layers.Dense(\n",
        "                arquitetura[i],\n",
        "                activation='relu',\n",
        "                input_shape=(dimensao_entrada,)\n",
        "            ))\n",
        "        elif i == len(arquitetura) - 1:\n",
        "            # Última camada - ativação baseada no tipo de problema\n",
        "            if tipo_problema == 'classificacao':\n",
        "                if dimensao_saida == 1:\n",
        "                    ativacao = 'sigmoid'\n",
        "                else:\n",
        "                    ativacao = 'softmax'\n",
        "            else:\n",
        "                ativacao = 'linear'\n",
        "            modelo.add(layers.Dense(arquitetura[i], activation=ativacao))\n",
        "        else:\n",
        "            # Camadas ocultas\n",
        "            modelo.add(layers.Dense(arquitetura[i], activation='relu'))\n",
        "            modelo.add(layers.Dropout(0.3))  # Dropout para regularização\n",
        "\n",
        "    # Compila modelo\n",
        "    if tipo_problema == 'classificacao':\n",
        "        if dimensao_saida == 1:\n",
        "            loss = 'binary_crossentropy'\n",
        "        else:\n",
        "            loss = 'categorical_crossentropy'\n",
        "    else:\n",
        "        loss = 'mse'\n",
        "\n",
        "    modelo.compile(\n",
        "        optimizer='adam',\n",
        "        loss=loss,\n",
        "        metrics=['accuracy'] if tipo_problema == 'classificacao' else ['mae']\n",
        "    )\n",
        "\n",
        "    print(\"    Modelo criado e compilado com sucesso!\")\n",
        "    modelo.summary()\n",
        "\n",
        "    return modelo\n",
        "\n",
        "# Exemplo de uso\n",
        "modelo_otimizado = criar_modelo_otimizado(\n",
        "    dimensao_entrada=15,\n",
        "    dimensao_saida=3,\n",
        "    n_amostras=1500,\n",
        "    tipo_problema='classificacao'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "jsxuyEzCvDoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 Funções de Ativação\n",
        "\n",
        "O que são: Funções que determinam a saída de cada neurônio.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Propósito: Introduzir não-linearidade na rede\n",
        "\n",
        "    Tipos principais:\n",
        "\n",
        "        ReLU: max(0,x) - Mais usada, evita vanishing gradient\n",
        "\n",
        "        Sigmoid: 1/(1+e⁻ˣ) - Para probabilidades (0-1)\n",
        "\n",
        "        Tanh: (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ) - Similar à sigmoid mas com média zero\n",
        "\n",
        "        Softmax: Transforma saídas em probabilidades (soma=1)\n",
        "\n",
        "    Escolha: Depende do problema e da camada"
      ],
      "metadata": {
        "id": "jaEr0DRRvHmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class VisualizacaoFuncoesAtivacao:\n",
        "    \"\"\"Visualiza diferentes funções de ativação\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def leaky_relu(x, alpha=0.1):\n",
        "        return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / np.sum(exp_x)\n",
        "\n",
        "    def visualizar(self):\n",
        "        \"\"\"Plota todas as funções de ativação\"\"\"\n",
        "        x = np.linspace(-5, 5, 100)\n",
        "\n",
        "        funcoes = {\n",
        "            'ReLU': self.relu(x),\n",
        "            'Sigmoid': self.sigmoid(x),\n",
        "            'Tanh': self.tanh(x),\n",
        "            'Leaky ReLU': self.leaky_relu(x),\n",
        "        }\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plota funções principais\n",
        "        for i, (nome, y) in enumerate(funcoes.items()):\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.plot(x, y, linewidth=2)\n",
        "            plt.title(nome, fontsize=14)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xlabel('x')\n",
        "            plt.ylabel('f(x)')\n",
        "\n",
        "        # Plota softmax (especial)\n",
        "        plt.subplot(2, 3, 5)\n",
        "        x_softmax = np.array([1.0, 2.0, 3.0])\n",
        "        y_softmax = self.softmax(x_softmax)\n",
        "        bars = plt.bar(range(len(x_softmax)), y_softmax)\n",
        "        plt.title('Softmax - Exemplo [1, 2, 3]', fontsize=14)\n",
        "        plt.xticks(range(len(x_softmax)), [f'Classe {i}' for i in range(len(x_softmax))])\n",
        "        plt.ylabel('Probabilidade')\n",
        "\n",
        "        # Adiciona valores nas barras\n",
        "        for bar, valor in zip(bars, y_softmax):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{valor:.2f}', ha='center', va='bottom')\n",
        "\n",
        "        # Explica características\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.axis('off')\n",
        "        info_text = \"\"\"\n",
        "        CARACTERÍSTICAS DAS FUNÇÕES:\n",
        "\n",
        "        ReLU:\n",
        "        • Simples e eficiente\n",
        "        • Evita vanishing gradient\n",
        "        • Pode causar \"neurônios mortos\"\n",
        "\n",
        "        Sigmoid:\n",
        "        • Saída entre 0 e 1\n",
        "        • Boa para probabilidades\n",
        "        • Sofre de vanishing gradient\n",
        "\n",
        "        Tanh:\n",
        "        • Saída entre -1 e 1\n",
        "        • Média zero\n",
        "        • Melhor que sigmoid\n",
        "\n",
        "        Leaky ReLU:\n",
        "        • Corrige \"neurônios mortos\"\n",
        "        • Permite pequenos valores negativos\n",
        "        \"\"\"\n",
        "        plt.text(0.1, 0.9, info_text, fontsize=10, va='top', linespacing=1.5)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle('Funções de Ativação em Redes Neurais', fontsize=16)\n",
        "        plt.show()\n",
        "\n",
        "print(\"=== VISUALIZAÇÃO DE FUNÇÕES DE ATIVAÇÃO ===\")\n",
        "visualizador = VisualizacaoFuncoesAtivacao()\n",
        "visualizador.visualizar()\n",
        "\n",
        "# Exemplo numérico\n",
        "print(\"\\n=== EXEMPLO NUMÉRICO ===\")\n",
        "x_exemplo = np.array([-2, -1, 0, 1, 2])\n",
        "print(f\"Entrada: {x_exemplo}\")\n",
        "\n",
        "funcoes_calc = {\n",
        "    'ReLU': visualizador.relu(x_exemplo),\n",
        "    'Sigmoid': visualizador.sigmoid(x_exemplo),\n",
        "    'Tanh': visualizador.tanh(x_exemplo),\n",
        "    'Leaky ReLU': visualizador.leaky_relu(x_exemplo),\n",
        "}\n",
        "\n",
        "for nome, valores in funcoes_calc.items():\n",
        "    print(f\"{nome:12}: {[f'{v:.3f}' for v in valores]}\")\n",
        "\n",
        "# Exemplo Softmax\n",
        "print(f\"\\nSoftmax exemplo:\")\n",
        "vetor = np.array([2.0, 1.0, 0.1])\n",
        "softmax_result = visualizador.softmax(vetor)\n",
        "print(f\"Entrada: {vetor}\")\n",
        "print(f\"Softmax: {softmax_result}\")\n",
        "print(f\"Soma: {np.sum(softmax_result):.1f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lKD3oQ5BvFzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3 Épocas, Batch Size e Learning Rate\n",
        "\n",
        "O que são: Hiperparâmetros críticos para o treinamento.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Épocas: Número de vezes que a rede vê todo o dataset\n",
        "\n",
        "        Poucas épocas: Underfitting\n",
        "\n",
        "        Muitas épocas: Overfitting\n",
        "\n",
        "    Batch Size: Número de amostras processadas antes da atualização dos pesos\n",
        "\n",
        "        Batch pequeno: Treino mais ruidoso mas generaliza melhor\n",
        "\n",
        "        Batch grande: Treino mais estável mas requer mais memória\n",
        "\n",
        "    Learning Rate: Tamanho do passo nas atualizações dos pesos\n",
        "\n",
        "        Muito alto: Pode divergir\n",
        "\n",
        "        Muito baixo: Treino muito lento"
      ],
      "metadata": {
        "id": "GaXTyVsUvMHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import callbacks\n",
        "import time\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "class EstrategiaTreino:\n",
        "    \"\"\"Classe completa com estratégias avançadas para configuração de treinamento\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calcular_epocas_ideais(n_amostras, complexidade='medio', tipo_problema='classificacao'):\n",
        "        \"\"\"\n",
        "        Calcula número sugerido de épocas baseado em múltiplos fatores\n",
        "\n",
        "        Args:\n",
        "            n_amostras (int): Número de amostras no dataset\n",
        "            complexidade (str): 'simples', 'medio', 'complexo'\n",
        "            tipo_problema (str): 'classificacao' ou 'regressao'\n",
        "\n",
        "        Returns:\n",
        "            int: Número sugerido de épocas\n",
        "        \"\"\"\n",
        "        print(f\" Calculando épocas ideais para:\")\n",
        "        print(f\"   Amostras: {n_amostras}\")\n",
        "        print(f\"   Complexidade: {complexidade}\")\n",
        "        print(f\"   Tipo: {tipo_problema}\")\n",
        "\n",
        "        # Base por complexidade\n",
        "        base_complexidade = {\n",
        "            'simples': 50,    # Problemas lineares simples\n",
        "            'medio': 100,     # Problemas não-lineares moderados\n",
        "            'complexo': 500   # Problemas muito complexos (imagens, etc)\n",
        "        }\n",
        "\n",
        "        # Ajuste por número de amostras\n",
        "        if n_amostras < 1000:\n",
        "            fator_amostras = 0.8\n",
        "        elif n_amostras < 10000:\n",
        "            fator_amostras = 1.0\n",
        "        else:\n",
        "            fator_amostras = 1.2\n",
        "\n",
        "        # Ajuste por tipo de problema\n",
        "        fator_tipo = 1.5 if tipo_problema == 'regressao' else 1.0\n",
        "\n",
        "        epocas_base = base_complexidade.get(complexidade, 100)\n",
        "        epocas_ajustadas = int(epocas_base * fator_amostras * fator_tipo)\n",
        "\n",
        "        # Limites práticos\n",
        "        epocas_final = max(10, min(epocas_ajustadas, 2000))\n",
        "\n",
        "        print(f\"    Base: {epocas_base}\")\n",
        "        print(f\"    Fator amostras: {fator_amostras}\")\n",
        "        print(f\"    Fator tipo: {fator_tipo}\")\n",
        "        print(f\"    Épocas sugeridas: {epocas_final}\")\n",
        "\n",
        "        return epocas_final\n",
        "\n",
        "    @staticmethod\n",
        "    def sugerir_batch_size(n_amostras, memoria_disponivel='medio', tipo_modelo='dense'):\n",
        "        \"\"\"\n",
        "        Sugere batch size baseado em múltiplos fatores\n",
        "\n",
        "        Args:\n",
        "            n_amostras (int): Número de amostras\n",
        "            memoria_disponivel (str): 'baixa', 'medio', 'alta'\n",
        "            tipo_modelo (str): 'dense', 'cnn', 'rnn'\n",
        "\n",
        "        Returns:\n",
        "            int: Batch size sugerido\n",
        "        \"\"\"\n",
        "        print(f\" Sugerindo batch size para:\")\n",
        "        print(f\"   Amostras: {n_amostras}\")\n",
        "        print(f\"   Memória: {memoria_disponivel}\")\n",
        "        print(f\"   Modelo: {tipo_modelo}\")\n",
        "\n",
        "        # Base por número de amostras\n",
        "        if n_amostras <= 500:\n",
        "            base = 16\n",
        "        elif n_amostras <= 1000:\n",
        "            base = 32\n",
        "        elif n_amostras <= 5000:\n",
        "            base = 64\n",
        "        elif n_amostras <= 20000:\n",
        "            base = 128\n",
        "        else:\n",
        "            base = 256\n",
        "\n",
        "        # Ajuste por memória disponível\n",
        "        fator_memoria = {\n",
        "            'baixa': 0.5,\n",
        "            'medio': 1.0,\n",
        "            'alta': 2.0\n",
        "        }.get(memoria_disponivel, 1.0)\n",
        "\n",
        "        # Ajuste por tipo de modelo\n",
        "        fator_modelo = {\n",
        "            'dense': 1.0,\n",
        "            'cnn': 0.5,    # CNNs geralmente usam batches menores\n",
        "            'rnn': 0.8     # RNNs podem precisar de batches menores devido à sequência\n",
        "        }.get(tipo_modelo, 1.0)\n",
        "\n",
        "        batch_ajustado = int(base * fator_memoria * fator_modelo)\n",
        "\n",
        "        # Garante que batch size é power of 2 (otimização GPU) e dentro de limites\n",
        "        batches_potencia_2 = [16, 32, 64, 128, 256, 512, 1024]\n",
        "        batch_final = min(batches_potencia_2,\n",
        "                         key=lambda x: abs(x - batch_ajustado))\n",
        "\n",
        "        # Não pode ser maior que número de amostras\n",
        "        batch_final = min(batch_final, n_amostras)\n",
        "\n",
        "        print(f\"    Base: {base}\")\n",
        "        print(f\"    Fator memória: {fator_memoria}\")\n",
        "        print(f\"    Fator modelo: {fator_modelo}\")\n",
        "        print(f\"    Batch size sugerido: {batch_final}\")\n",
        "\n",
        "        return batch_final\n",
        "\n",
        "    @staticmethod\n",
        "    def scheduler_learning_rate(epoca, lr_inicial=0.001, esquema='decay'):\n",
        "        \"\"\"\n",
        "        Scheduling de learning rate com múltiplas estratégias\n",
        "\n",
        "        Args:\n",
        "            epoca (int): Época atual\n",
        "            lr_inicial (float): Learning rate inicial\n",
        "            esquema (str): 'decay', 'step', 'cosine', 'warmup'\n",
        "\n",
        "        Returns:\n",
        "            float: Learning rate para a época\n",
        "        \"\"\"\n",
        "        if esquema == 'decay':\n",
        "            # Decaimento exponencial suave\n",
        "            lr = lr_inicial * np.exp(-0.1 * (epoca // 10))\n",
        "        elif esquema == 'step':\n",
        "            # Redução em degraus\n",
        "            if epoca < 10:\n",
        "                lr = lr_inicial\n",
        "            elif epoca < 50:\n",
        "                lr = lr_inicial * 0.1\n",
        "            elif epoca < 100:\n",
        "                lr = lr_inicial * 0.01\n",
        "            else:\n",
        "                lr = lr_inicial * 0.001\n",
        "        elif esquema == 'cosine':\n",
        "            # Decaimento cosseno (popular em papers recentes)\n",
        "            lr = lr_inicial * (1 + np.cos(np.pi * epoca / 200)) / 2\n",
        "        elif esquema == 'warmup':\n",
        "            # Warmup seguido de decay\n",
        "            if epoca < 5:\n",
        "                lr = lr_inicial * (epoca + 1) / 5  # Warmup linear\n",
        "            else:\n",
        "                lr = lr_inicial * np.exp(-0.1 * ((epoca - 5) // 10))\n",
        "        else:\n",
        "            lr = lr_inicial\n",
        "\n",
        "        return max(lr, 1e-6)  # Learning rate mínimo\n",
        "\n",
        "    @staticmethod\n",
        "    def criar_callbacks_avancados(monitor='val_loss', patience=15, lr_patience=10, verbose=1):\n",
        "        \"\"\"\n",
        "        Cria conjunto completo de callbacks para treinamento robusto\n",
        "\n",
        "        Args:\n",
        "            monitor (str): Métrica para monitorar\n",
        "            patience (int): Paciência para early stopping\n",
        "            lr_patience (int): Paciência para redução de LR\n",
        "            verbose (int): Verbosidade\n",
        "\n",
        "        Returns:\n",
        "            list: Lista de callbacks do Keras\n",
        "        \"\"\"\n",
        "        callbacks_list = [\n",
        "            # Early Stopping - para treino se não houver melhoria\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                patience=patience,\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose\n",
        "            ),\n",
        "\n",
        "            # ReduceLROnPlateau - reduz LR quando estagna\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                factor=0.5,\n",
        "                patience=lr_patience,\n",
        "                min_lr=1e-7,\n",
        "                verbose=verbose\n",
        "            ),\n",
        "\n",
        "            # Model Checkpoint - salva melhores modelos\n",
        "            callbacks.ModelCheckpoint(\n",
        "                filepath='melhor_modelo.h5',\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                verbose=verbose\n",
        "            ),\n",
        "\n",
        "            # TensorBoard - para visualização (opcional)\n",
        "            # callbacks.TensorBoard(\n",
        "            #     log_dir='./logs',\n",
        "            #     histogram_freq=1\n",
        "            # ),\n",
        "\n",
        "            # CSV Logger - salva histórico em CSV\n",
        "            callbacks.CSVLogger(\n",
        "                'historico_treino.csv',\n",
        "                separator=',',\n",
        "                append=False\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        print(\"  Callbacks configurados:\")\n",
        "        print(\"    Early Stopping\")\n",
        "        print(\"    Reduce LR on Plateau\")\n",
        "        print(\"    Model Checkpoint\")\n",
        "        print(\"    CSV Logger\")\n",
        "\n",
        "        return callbacks_list\n",
        "\n",
        "    @staticmethod\n",
        "    def analisar_curva_aprendizado(historico, tamanho_testes=None):\n",
        "        \"\"\"\n",
        "        Analisa e visualiza a curva de aprendizado\n",
        "\n",
        "        Args:\n",
        "            historico: Objeto history retornado pelo model.fit()\n",
        "            tamanho_testes (list): Tamanhos de dataset para análise\n",
        "\n",
        "        Returns:\n",
        "            dict: Análise da curva de aprendizado\n",
        "        \"\"\"\n",
        "        if hasattr(historico, 'history'):\n",
        "            history = historico.history\n",
        "        else:\n",
        "            history = historico\n",
        "\n",
        "        print(\"📊 Analisando curva de aprendizado...\")\n",
        "\n",
        "        # Métricas disponíveis\n",
        "        metricas = [k for k in history.keys() if not k.startswith('val_')]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Análise Completa da Curva de Aprendizado', fontsize=16)\n",
        "\n",
        "        analise = {}\n",
        "\n",
        "        for i, metrica in enumerate(metricas[:2]):  # Analisa até 2 métricas\n",
        "            if metrica in history:\n",
        "                # Plot da métrica de treino e validação\n",
        "                ax = axes[i, 0]\n",
        "                ax.plot(history[metrica], label=f'Treino {metrica}', linewidth=2)\n",
        "\n",
        "                val_metrica = f'val_{metrica}'\n",
        "                if val_metrica in history:\n",
        "                    ax.plot(history[val_metrica], label=f'Validação {metrica}', linewidth=2)\n",
        "\n",
        "                ax.set_title(f'Curva de {metrica.upper()}')\n",
        "                ax.set_xlabel('Época')\n",
        "                ax.set_ylabel(metrica)\n",
        "                ax.legend()\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "                # Análise de convergência\n",
        "                if len(history[metrica]) > 10:\n",
        "                    ultimos_10 = history[metrica][-10:]\n",
        "                    variacao = np.std(ultimos_10) / np.mean(ultimos_10)\n",
        "\n",
        "                    analise[metrica] = {\n",
        "                        'convergiu': variacao < 0.05,\n",
        "                        'variacao_ultimas_epocas': variacao,\n",
        "                        'melhor_epoca': np.argmin(history[metrica]) if 'loss' in metrica else np.argmax(history[metrica])\n",
        "                    }\n",
        "\n",
        "        # Análise de overfitting\n",
        "        if 'loss' in history and 'val_loss' in history:\n",
        "            ax = axes[0, 1]\n",
        "            overfitting_ratio = []\n",
        "            for i in range(min(len(history['loss']), len(history['val_loss']))):\n",
        "                ratio = history['loss'][i] / history['val_loss'][i] if history['val_loss'][i] > 0 else 1\n",
        "                overfitting_ratio.append(ratio)\n",
        "\n",
        "            ax.plot(overfitting_ratio, color='red', linewidth=2)\n",
        "            ax.axhline(y=1.0, color='black', linestyle='--', label='Ideal')\n",
        "            ax.axhline(y=1.5, color='orange', linestyle='--', label='Limite Moderado')\n",
        "            ax.axhline(y=2.0, color='red', linestyle='--', label='Limite Severo')\n",
        "            ax.set_title('Ratio Overfitting (Treino/Validação)')\n",
        "            ax.set_xlabel('Época')\n",
        "            ax.set_ylabel('Ratio Loss Treino/Validação')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Classificação do overfitting\n",
        "            ultimo_ratio = overfitting_ratio[-1] if overfitting_ratio else 1\n",
        "            if ultimo_ratio < 1.2:\n",
        "                status_overfitting = \"BAIXO\"\n",
        "            elif ultimo_ratio < 1.8:\n",
        "                status_overfitting = \"MODERADO\"\n",
        "            else:\n",
        "                status_overfitting = \"ALTO\"\n",
        "\n",
        "            analise['overfitting'] = {\n",
        "                'status': status_overfitting,\n",
        "                'ratio_final': ultimo_ratio\n",
        "            }\n",
        "\n",
        "        # Learning rate (se disponível)\n",
        "        if 'lr' in history:\n",
        "            ax = axes[1, 1]\n",
        "            ax.plot(history['lr'], color='purple', linewidth=2)\n",
        "            ax.set_title('Evolução do Learning Rate')\n",
        "            ax.set_xlabel('Época')\n",
        "            ax.set_ylabel('Learning Rate')\n",
        "            ax.set_yscale('log')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Relatório de análise\n",
        "        print(\"\\n RELATÓRIO DA ANÁLISE:\")\n",
        "        for metrica, info in analise.items():\n",
        "            if metrica != 'overfitting':\n",
        "                status = \"CONVERGIU\" if info['convergiu'] else \"NÃO CONVERGIU\"\n",
        "                print(f\"    {metrica.upper()}: {status} (variação: {info['variacao_ultimas_epocas']:.3f})\")\n",
        "\n",
        "        if 'overfitting' in analise:\n",
        "            print(f\"    OVERFITTING: {analise['overfitting']['status']} (ratio: {analise['overfitting']['ratio_final']:.2f})\")\n",
        "\n",
        "        return analise\n",
        "\n",
        "    @staticmethod\n",
        "    def otimizar_hiperparametros(n_amostras, complexidade='medio', tipo_problema='classificacao'):\n",
        "        \"\"\"\n",
        "        Sugere configurações completas de hiperparâmetros\n",
        "\n",
        "        Args:\n",
        "            n_amostras (int): Número de amostras\n",
        "            complexidade (str): Complexidade do problema\n",
        "            tipo_problema (str): Tipo de problema\n",
        "\n",
        "        Returns:\n",
        "            dict: Configurações otimizadas\n",
        "        \"\"\"\n",
        "        print(\"  Otimizando hiperparâmetros...\")\n",
        "\n",
        "        # Épocas\n",
        "        epocas = EstrategiaTreino.calcular_epocas_ideais(n_amostras, complexidade, tipo_problema)\n",
        "\n",
        "        # Batch size\n",
        "        batch_size = EstrategiaTreino.sugerir_batch_size(n_amostras)\n",
        "\n",
        "        # Learning rate baseado na complexidade\n",
        "        lr_base = {\n",
        "            'simples': 0.01,\n",
        "            'medio': 0.001,\n",
        "            'complexo': 0.0001\n",
        "        }.get(complexidade, 0.001)\n",
        "\n",
        "        # Otimizador recomendado\n",
        "        if complexidade == 'simples':\n",
        "            otimizador = 'sgd'\n",
        "        elif complexidade == 'medio':\n",
        "            otimizador = 'adam'\n",
        "        else:\n",
        "            otimizador = 'adam'\n",
        "\n",
        "        config = {\n",
        "            'epocas': epocas,\n",
        "            'batch_size': batch_size,\n",
        "            'learning_rate': lr_base,\n",
        "            'otimizador': otimizador,\n",
        "            'callbacks': EstrategiaTreino.criar_callbacks_avancados(),\n",
        "            'estrategia_lr': 'decay' if complexidade == 'complexo' else 'step'\n",
        "        }\n",
        "\n",
        "        print(\" Configurações otimizadas:\")\n",
        "        for chave, valor in config.items():\n",
        "            if chave != 'callbacks':\n",
        "                print(f\"   {chave}: {valor}\")\n",
        "\n",
        "        return config\n",
        "\n",
        "    @staticmethod\n",
        "    def calcular_tempo_estimado_treino(n_amostras, batch_size, epocas, complexidade_modelo='medio'):\n",
        "        \"\"\"\n",
        "        Estima tempo de treinamento baseado em características do problema\n",
        "\n",
        "        Args:\n",
        "            n_amostras (int): Número de amostras\n",
        "            batch_size (int): Tamanho do batch\n",
        "            epocas (int): Número de épocas\n",
        "            complexidade_modelo (str): Complexidade do modelo\n",
        "\n",
        "        Returns:\n",
        "            dict: Estimativa de tempo em diferentes cenários\n",
        "        \"\"\"\n",
        "        # batches por época\n",
        "        batches_por_epoca = n_amostras // batch_size\n",
        "\n",
        "        # Tempo por batch (segundos) - estimativa base\n",
        "        tempo_por_batch_base = {\n",
        "            'simples': 0.01,   # Modelos simples\n",
        "            'medio': 0.05,     # Modelos médios\n",
        "            'complexo': 0.2    # Modelos complexos (CNNs, etc)\n",
        "        }.get(complexidade_modelo, 0.05)\n",
        "\n",
        "        # Tempo total estimado\n",
        "        tempo_total_epocas = batches_por_epoca * tempo_por_batch_base * epocas\n",
        "\n",
        "        # Conversão para minutos/horas\n",
        "        tempo_minutos = tempo_total_epocas / 60\n",
        "        tempo_horas = tempo_minutos / 60\n",
        "\n",
        "        estimativa = {\n",
        "            'segundos_total': tempo_total_epocas,\n",
        "            'minutos_total': tempo_minutos,\n",
        "            'horas_total': tempo_horas,\n",
        "            'segundos_por_epoca': tempo_total_epocas / epocas,\n",
        "            'batches_por_epoca': batches_por_epoca\n",
        "        }\n",
        "\n",
        "        print(\"  ESTIMATIVA DE TEMPO DE TREINO:\")\n",
        "        print(f\"    Épocas: {epocas}\")\n",
        "        print(f\"    Batches/época: {batches_por_epoca}\")\n",
        "        print(f\"    Tempo total estimado: {tempo_minutos:.1f} minutos ({tempo_horas:.2f} horas)\")\n",
        "        print(f\"    Tempo por época: {estimativa['segundos_por_epoca']:.1f} segundos\")\n",
        "\n",
        "        return estimativa\n",
        "\n",
        "\n",
        "# EXEMPLOS PRÁTICOS\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 1: PROBLEMA DE CLASSIFICAÇÃO MÉDIA\")\n",
        "\n",
        "\n",
        "# Cenário: Classificação com dataset médio\n",
        "n_amostras = 5000\n",
        "complexidade = 'medio'\n",
        "tipo_problema = 'classificacao'\n",
        "\n",
        "print(f\" CENÁRIO: Classificação com {n_amostras} amostras\")\n",
        "\n",
        "# 1. Obter configurações otimizadas\n",
        "config = EstrategiaTreino.otimizar_hiperparametros(\n",
        "    n_amostras=n_amostras,\n",
        "    complexidade=complexidade,\n",
        "    tipo_problema=tipo_problema\n",
        ")\n",
        "\n",
        "# 2. Calcular tempo estimado\n",
        "tempo_estimado = EstrategiaTreino.calcular_tempo_estimado_treino(\n",
        "    n_amostras=n_amostras,\n",
        "    batch_size=config['batch_size'],\n",
        "    epocas=config['epocas'],\n",
        "    complexidade_modelo=complexidade\n",
        ")\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 2: COMPARAÇÃO DE BATCH SIZES\")\n",
        "\n",
        "\n",
        "# Comparar diferentes batch sizes\n",
        "n_amostras_test = 10000\n",
        "memorias = ['baixa', 'medio', 'alta']\n",
        "modelos = ['dense', 'cnn', 'rnn']\n",
        "\n",
        "print(\" COMPARANDO BATCH SIZES:\")\n",
        "for memoria in memorias:\n",
        "    for modelo in modelos:\n",
        "        batch = EstrategiaTreino.sugerir_batch_size(\n",
        "            n_amostras_test, memoria, modelo\n",
        "        )\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 3: ESTRATÉGIAS DE LEARNING RATE\")\n",
        "\n",
        "\n",
        "# Comparar diferentes estratégias de LR\n",
        "epocas_test = 150\n",
        "lr_inicial = 0.01\n",
        "estrategias = ['decay', 'step', 'cosine', 'warmup']\n",
        "\n",
        "print(\" COMPARANDO ESTRATÉGIAS DE LEARNING RATE:\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for estrategia in estrategias:\n",
        "    lr_history = []\n",
        "    for epoca in range(epocas_test):\n",
        "        lr = EstrategiaTreino.scheduler_learning_rate(\n",
        "            epoca, lr_inicial, estrategia\n",
        "        )\n",
        "        lr_history.append(lr)\n",
        "\n",
        "    plt.plot(lr_history, label=estrategia, linewidth=2)\n",
        "\n",
        "plt.title('Comparação de Estratégias de Learning Rate')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 4: SIMULAÇÃO DE TREINAMENTO COMPLETO\")\n",
        "\n",
        "\n",
        "# Simular um histórico de treinamento para análise\n",
        "print(\" Simulando histórico de treinamento...\")\n",
        "\n",
        "# Histórico simulado (tipico de um treinamento)\n",
        "historico_simulado = {\n",
        "    'loss': [0.8, 0.5, 0.3, 0.2, 0.15, 0.12, 0.1, 0.09, 0.085, 0.082, 0.08, 0.079, 0.078, 0.077, 0.076],\n",
        "    'val_loss': [0.7, 0.45, 0.28, 0.18, 0.14, 0.13, 0.12, 0.115, 0.11, 0.108, 0.107, 0.106, 0.105, 0.104, 0.103],\n",
        "    'accuracy': [0.65, 0.75, 0.82, 0.88, 0.91, 0.92, 0.93, 0.935, 0.94, 0.942, 0.945, 0.947, 0.948, 0.949, 0.95],\n",
        "    'val_accuracy': [0.68, 0.77, 0.83, 0.87, 0.89, 0.90, 0.905, 0.908, 0.91, 0.912, 0.913, 0.914, 0.915, 0.916, 0.917],\n",
        "    'lr': [0.001] * 15  # LR constante para exemplo\n",
        "}\n",
        "\n",
        "# Analisar curva de aprendizado\n",
        "analise = EstrategiaTreino.analisar_curva_aprendizado(historico_simulado)\n",
        "\n",
        "\n",
        "print(\" RESUMO DAS ESTRATÉGIAS RECOMENDADAS\")\n",
        "\n",
        "\n",
        "estrategias_resumo = {\n",
        "    \"Batch Size\": \"Use potências de 2 (32, 64, 128...) para otimização GPU\",\n",
        "    \"Learning Rate\": \"Comece com 0.001-0.01 e use scheduling\",\n",
        "    \"Early Stopping\": \"Monitore val_loss com patience 10-20\",\n",
        "    \"Callbacks\": \"Sempre use ReduceLROnPlateau e ModelCheckpoint\",\n",
        "    \"Épocas\": \"Ajuste baseado em complexidade e tamanho do dataset\",\n",
        "    \"Overfitting\": \"Ratio treino/validação > 1.5 indica overfitting\"\n",
        "}\n",
        "\n",
        "for estrategia, recomendacao in estrategias_resumo.items():\n",
        "    print(f\"    {estrategia}: {recomendacao}\")\n",
        "\n",
        "# EXEMPLO 5: IMPLEMENTAÇÃO PRÁTICA COM TENSORFLOW\n",
        "\n",
        "\n",
        "\n",
        "print(\" EXEMPLO 5: IMPLEMENTAÇÃO COM TENSORFLOW\")\n",
        "\n",
        "\n",
        "def criar_modelo_exemplo(input_dim=20, output_dim=3):\n",
        "    \"\"\"Cria modelo de exemplo para demonstração\"\"\"\n",
        "    modelo = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return modelo\n",
        "\n",
        "# Configurações para um problema específico\n",
        "n_amostras_exemplo = 8000\n",
        "input_dim = 20\n",
        "output_dim = 3\n",
        "\n",
        "print(f\" CRIANDO PIPELINE DE TREINO PARA:\")\n",
        "print(f\"   Amostras: {n_amostras_exemplo}\")\n",
        "print(f\"   Input: {input_dim}D, Output: {output_dim}D\")\n",
        "\n",
        "# 1. Obter configurações otimizadas\n",
        "config_exemplo = EstrategiaTreino.otimizar_hiperparametros(\n",
        "    n_amostras=n_amostras_exemplo,\n",
        "    complexidade='medio',\n",
        "    tipo_problema='classificacao'\n",
        ")\n",
        "\n",
        "# 2. Criar modelo\n",
        "modelo_exemplo = criar_modelo_exemplo(input_dim, output_dim)\n",
        "\n",
        "# 3. Compilar com configurações otimizadas\n",
        "modelo_exemplo.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=config_exemplo['learning_rate']),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\" Modelo compilado com configurações otimizadas!\")\n",
        "print(f\"   Otimizador: {config_exemplo['otimizador']}\")\n",
        "print(f\"   Learning Rate: {config_exemplo['learning_rate']}\")\n",
        "\n",
        "# 4. Dados de exemplo (simulados)\n",
        "X_simulado = np.random.randn(n_amostras_exemplo, input_dim)\n",
        "y_simulado = np.random.randint(0, output_dim, n_amostras_exemplo)\n",
        "\n",
        "print(f\" Dados simulados: X {X_simulado.shape}, y {y_simulado.shape}\")\n",
        "\n",
        "# 5. Treinar com callbacks (comentado para não executar de verdade)\n",
        "print(\" Treinamento comentado para demonstração:\")\n",
        "print(\"\"\"\n",
        "# historico = modelo_exemplo.fit(\n",
        "#     X_simulado, y_simulado,\n",
        "#     epochs=config_exemplo['epocas'],\n",
        "#     batch_size=config_exemplo['batch_size'],\n",
        "#     validation_split=0.2,\n",
        "#     callbacks=config_exemplo['callbacks'],\n",
        "#     verbose=1\n",
        "# )\n",
        "\"\"\")\n",
        "\n",
        "print(\" Pipeline de treino configurado com sucesso!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "m8_JbAVCvLuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Bibliotecas Principais"
      ],
      "metadata": {
        "id": "s-1-MQLvvSHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "O que é: Framework mais popular para deep learning.\n",
        "\n",
        "# TensorFlow/Keras\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    TensorFlow: Framework de baixo nível com execução eficiente\n",
        "\n",
        "    Keras: API de alto nível que simplifica o uso do TensorFlow\n",
        "\n",
        "    Vantagens:\n",
        "\n",
        "        Comunidade grande e ativa\n",
        "\n",
        "        Documentação extensa\n",
        "\n",
        "        Suporte a GPU/TPU\n",
        "\n",
        "        Produção-ready\n",
        "\n",
        "    Casos de uso: Projetos de pequena a grande escala\n",
        "\n",
        "# Pytorch:\n",
        "    \n",
        "    Características:\n",
        "\n",
        "    Computação dinâmica (define-by-run)\n",
        "\n",
        "    Interface Pythonica\n",
        "\n",
        "    Boa para prototipagem rápida\n",
        "\n",
        "Vantagens:\n",
        "\n",
        "    Flexibilidade\n",
        "\n",
        "    Debugging fácil\n",
        "\n",
        "    Crescimento rápido na indústria\n",
        "\n",
        "Casos de uso: Pesquisa, prototipagem, projetos acadêmicos\n",
        "\n",
        "\n",
        "# Scikit-Learn\n",
        "\n",
        "Vantagens:\n",
        "\n",
        "    Interface consistente\n",
        "\n",
        "    Integração com outras técnicas de ML\n",
        "\n",
        "    Boa para baseline\n",
        "\n",
        "Limitações:\n",
        "\n",
        "    Não suporta redes complexas\n",
        "\n",
        "    Menos eficiente para grandes datasets\n",
        "\n",
        "Casos de uso: Problemas simples, prototipagem rápida\n",
        "\n"
      ],
      "metadata": {
        "id": "WF_Ip2RSvVcw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo: Scikit-Learn"
      ],
      "metadata": {
        "id": "_ZBcOlXIEIGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=== REDES NEURAIS COM SCIKIT-LEARN ===\")\n",
        "\n",
        "# Cria dataset moons (problema não linear)\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "print(f\"Dataset criado: {X.shape} amostras, {len(np.unique(y))} classes\")\n",
        "\n",
        "# Visualiza os dados\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', alpha=0.6, label='Classe 0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', alpha=0.6, label='Classe 1')\n",
        "plt.title('Dataset Moons - Dados Originais')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Pré-processamento\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Divide em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42\n",
        ")\n",
        "print(f\"Treino: {X_train.shape}, Teste: {X_test.shape}\")\n",
        "\n",
        "# Cria e treina MLP\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),  # 2 camadas: 100 e 50 neurônios\n",
        "    activation='relu',             # Função de ativação\n",
        "    solver='adam',                 # Otimizador\n",
        "    alpha=0.001,                   # Termo de regularização\n",
        "    learning_rate_init=0.001,      # Taxa de aprendizado\n",
        "    max_iter=500,                  # Máximo de iterações\n",
        "    random_state=42,\n",
        "    early_stopping=True,           # Para early se não melhorar\n",
        "    validation_fraction=0.1        # 10% para validação\n",
        ")\n",
        "\n",
        "print(\"\\n=== TREINANDO MODELO ===\")\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Treinamento concluído após {mlp.n_iter_} iterações\")\n",
        "print(f\"Loss final: {mlp.loss_:.4f}\")\n",
        "\n",
        "# Avaliação\n",
        "y_pred = mlp.predict(X_test)\n",
        "accuracy = mlp.score(X_test, y_test)\n",
        "print(f\"\\n=== AVALIAÇÃO ===\")\n",
        "print(f\"Acurácia no teste: {accuracy:.4f}\")\n",
        "\n",
        "# Métricas detalhadas\n",
        "print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"=== MATRIZ DE CONFUSÃO ===\")\n",
        "print(cm)\n",
        "\n",
        "# Visualiza fronteira de decisão\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Cria grid para plotar fronteira\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(X_scaled[:, 0].min() - 0.5, X_scaled[:, 0].max() + 0.5, 100),\n",
        "    np.linspace(X_scaled[:, 1].min() - 0.5, X_scaled[:, 1].max() + 0.5, 100)\n",
        ")\n",
        "\n",
        "# Previsões no grid\n",
        "Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plota fronteira\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1],\n",
        "           color='red', alpha=0.6, label='Classe 0 Real')\n",
        "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1],\n",
        "           color='blue', alpha=0.6, label='Classe 1 Real')\n",
        "\n",
        "# Destaca erros de classificação\n",
        "erros = y_test != y_pred\n",
        "plt.scatter(X_test[erros, 0], X_test[erros, 1],\n",
        "           color='black', marker='x', s=100, label='Erros')\n",
        "\n",
        "plt.title(f'Fronteira de Decisão MLP\\nAcurácia: {accuracy:.3f}')\n",
        "plt.xlabel('Feature 1 (escalada)')\n",
        "plt.ylabel('Feature 2 (escalada)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Informações da rede\n",
        "print(\"\\n=== INFORMAÇÕES DA REDE ===\")\n",
        "print(f\"Número de camadas: {mlp.n_layers_}\")\n",
        "print(f\"Neurônios por camada: {mlp.hidden_layer_sizes}\")\n",
        "print(f\"Função de ativação: {mlp.activation}\")\n",
        "print(f\"Otimizador: {mlp.solver}\")\n",
        "\n",
        "# Curva de aprendizado\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.title('Curva de Aprendizado do MLP')\n",
        "plt.xlabel('Iteração')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t5KuF5hQvkWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Técnicas Avançadas"
      ],
      "metadata": {
        "id": "EA2JqT7gv7uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Regularização\n",
        "\n",
        "O que é: Técnicas para prevenir overfitting.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    L1/L2 Regularization: Adiciona penalidade aos pesos grandes\n",
        "\n",
        "    Dropout: Desliga neurônios aleatoriamente durante o treino\n",
        "\n",
        "    Batch Normalization: Normaliza as ativações de cada camada\n",
        "\n",
        "    Early Stopping: Para o treino quando a validação para de melhorar\n",
        "\n",
        "    Data Augmentation: Cria variações dos dados de treino"
      ],
      "metadata": {
        "id": "_8pMDln4wAV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TecnicasRegularizacao:\n",
        "    \"\"\"Implementação de técnicas de regularização\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def criar_modelo_regularizado(dimensao_entrada):\n",
        "        # Cria modelo com várias técnicas de regularização\n",
        "        modelo = models.Sequential([\n",
        "            # Camada com regularização L2 nos pesos\n",
        "            layers.Dense(64, activation='relu', input_shape=(dimensao_entrada,),\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "            layers.BatchNormalization(),  # Normalização em lote\n",
        "            layers.Dropout(0.3),          # Dropout 30%\n",
        "\n",
        "            # Segunda camada regularizada\n",
        "            layers.Dense(32, activation='relu',\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.3),\n",
        "\n",
        "            # Camada de saída\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        return modelo\n",
        "\n",
        "    @staticmethod\n",
        "    def data_augmentation_imagens():\n",
        "        \"\"\"Data augmentation para imagens - cria variações dos dados\"\"\"\n",
        "        data_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "            rotation_range=20,       # Rotação aleatória até 20 graus\n",
        "            width_shift_range=0.2,   # Deslocamento horizontal aleatório\n",
        "            height_shift_range=0.2,  # Deslocamento vertical aleatório\n",
        "            horizontal_flip=True,    # Inversão horizontal aleatória\n",
        "            zoom_range=0.2,          # Zoom aleatório\n",
        "            shear_range=0.2,         # Cisalhamento aleatório\n",
        "            fill_mode='nearest'      # Preenche pixels faltantes com vizinhos\n",
        "        )\n",
        "        return data_gen"
      ],
      "metadata": {
        "id": "uNDl3wXswC9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Transfer Learning\n",
        "\n",
        "O que é: Reutilizar modelos pré-treinados em novos problemas.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Como funciona:\n",
        "\n",
        "        Usa pesos de redes treinadas em grandes datasets (ex: ImageNet)\n",
        "\n",
        "        Fine-tuning nas camadas finais para a tarefa específica\n",
        "\n",
        "    Vantagens:\n",
        "\n",
        "        Requer menos dados\n",
        "\n",
        "        Treino mais rápido\n",
        "\n",
        "        Melhor performance\n",
        "\n",
        "    Aplicações:\n",
        "\n",
        "        Visão computacional\n",
        "\n",
        "        Processamento de linguagem natural\n",
        "\n"
      ],
      "metadata": {
        "id": "SHklIo24wDy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransferLearningExample:\n",
        "    \"\"\"Exemplo de Transfer Learning\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def criar_modelo_transfer_learning():\n",
        "        # Usa modelo pré-treinado MobileNetV2\n",
        "        base_model = tf.keras.applications.MobileNetV2(\n",
        "            weights='imagenet',       # Pesos treinados no ImageNet\n",
        "            include_top=False,        # Exclui camadas fully connected do topo\n",
        "            input_shape=(224, 224, 3) # Forma de entrada esperada\n",
        "        )\n",
        "\n",
        "        # Congela camadas base (não treina)\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Adiciona novas camadas para tarefa específica\n",
        "        modelo = models.Sequential([\n",
        "            base_model,                      # Base pré-treinada\n",
        "            layers.GlobalAveragePooling2D(), # Pooling para reduzir dimensionalidade\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(10, activation='softmax')  # 10 classes para novo problema\n",
        "        ])\n",
        "\n",
        "        return modelo"
      ],
      "metadata": {
        "id": "OKkTwIftwH1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Ferramentas de Análise e Visualização\n",
        "\n",
        "O que são: Técnicas para entender e depurar redes neurais.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Histórico de Treino: Gráficos de loss e acurácia ao longo do tempo\n",
        "\n",
        "    Visualização de Arquitetura: Diagramas da estrutura da rede\n",
        "\n",
        "    Análise de Camadas: Visualização das ativações internas\n",
        "\n",
        "    Matriz de Confusão: Performance detalhada por classe\n",
        "\n",
        "    Curvas ROC: Trade-off entre verdadeiros positivos e falsos positivos"
      ],
      "metadata": {
        "id": "rUm4ASQ-wM_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class VisualizacaoRedesNeurais:\n",
        "    \"\"\"Ferramentas para visualização e análise\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def plotar_historico_treino(historico):\n",
        "        # Cria figura com 2 subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        # Plot loss\n",
        "        ax1.plot(historico.history['loss'], label='Treino')\n",
        "        ax1.plot(historico.history['val_loss'], label='Validação')\n",
        "        ax1.set_title('Loss durante Treino')\n",
        "        ax1.set_xlabel('Época')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "\n",
        "        # Plot acurácia (se disponível)\n",
        "        if 'accuracy' in historico.history:\n",
        "            ax2.plot(historico.history['accuracy'], label='Treino')\n",
        "            ax2.plot(historico.history['val_accuracy'], label='Validação')\n",
        "            ax2.set_title('Acurácia durante Treino')\n",
        "            ax2.set_xlabel('Época')\n",
        "            ax2.set_ylabel('Acurácia')\n",
        "            ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def visualizar_arquitetura(modelo):\n",
        "        # Gera visualização da arquitetura do modelo\n",
        "        tf.keras.utils.plot_model(\n",
        "            modelo,\n",
        "            to_file='model_architecture.png',  # Salva em arquivo\n",
        "            show_shapes=True,                  # Mostra formas dos tensores\n",
        "            show_layer_names=True              # Mostra nomes das camadas\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def analisar_camadas(modelo, X_exemplo):\n",
        "        \"\"\"Analisa ativações das camadas para entender o que a rede aprende\"\"\"\n",
        "        # Cria modelo que retorna saídas de todas as camadas\n",
        "        saidas_camadas = [camada.output for camada in modelo.layers]\n",
        "        modelo_ativacoes = models.Model(\n",
        "            inputs=modelo.input,\n",
        "            outputs=saidas_camadas\n",
        "        )\n",
        "\n",
        "        # Calcula ativações para exemplo de entrada\n",
        "        ativacoes = modelo_ativacoes.predict(X_exemplo)\n",
        "\n",
        "        # Plota ativações de cada camada\n",
        "        for i, ativacao in enumerate(ativacoes):\n",
        "            if len(ativacao.shape) == 2:  # Camadas densas\n",
        "                plt.figure(figsize=(8, 4))\n",
        "                plt.imshow(ativacao, aspect='auto', cmap='viridis')\n",
        "                plt.title(f'Camada {i} - {modelo.layers[i].name}')\n",
        "                plt.colorbar()\n",
        "                plt.show()"
      ],
      "metadata": {
        "id": "l8mlnZjWwL46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Considerações Práticas"
      ],
      "metadata": {
        "id": "YiNSV46_wWUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Dicas para Treino Eficiente\n",
        "\n",
        "O que são: Melhores práticas baseadas em experiência.\n",
        "\n",
        "Explicação:\n",
        "\n",
        "    Pré-processamento: Sempre normalize os dados\n",
        "\n",
        "    Validação: Use split treino/validação/teste\n",
        "\n",
        "    Hiperparâmetros: Comece com valores padrão e ajuste gradualmente\n",
        "\n",
        "    Monitoramento: Acompanhe métricas durante o treino\n",
        "\n",
        "    Experimentação: Mantenha um log de experimentos"
      ],
      "metadata": {
        "id": "S0fxxCQewZP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Glossário de Termos Técnicos\n",
        "\n",
        "* Backpropagation: Algoritmo para calcular gradientes eficientemente\n",
        "\n",
        "* Gradiente: Direção e magnitude da mudança necessária nos pesos\n",
        "\n",
        "* Loss Function: Função que mede o erro da rede\n",
        "\n",
        "* Optimizer: Algoritmo que atualiza os pesos baseado nos gradientes\n",
        "\n",
        "* Forward Pass: Cálculo da saída da rede para uma dada entrada\n",
        "\n",
        "* Batch: Conjunto de amostras processadas juntas\n",
        "\n",
        "* Epoch: Passagem completa pelo dataset de treino\n",
        "\n",
        "* Validation: Avaliação durante o treino para monitorar generalização\n",
        "\n",
        "* Teste: Avaliação final em dados nunca vistos\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nWWrF9xxzaY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oficina de Redes Neurais - Referências Completas\n",
        "\n",
        "##  Referências Acadêmicas e Históricas\n",
        "\n",
        "### Livros Fundamentais\n",
        "\n",
        "#### 1. **Referências Clássicas**\n",
        "- **McCulloch, W. S., & Pitts, W. (1943).** \"A logical calculus of the ideas immanent in nervous activity\". *Bulletin of Mathematical Biophysics*.\n",
        "  - **Importância:** Artigo seminal que introduziu o primeiro modelo matemático de neurônio\n",
        "\n",
        "- **Rosenblatt, F. (1958).** \"The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain\". *Psychological Review*.\n",
        "  - **Contribuição:** Introduziu o conceito de perceptron e aprendizado supervisionado\n",
        "\n",
        "- **Minsky, M., & Papert, S. (1969).** \"Perceptrons: An Introduction to Computational Geometry\".\n",
        "  - **Impacto:** Demonstrou limitações do perceptron simples, levando ao \"inverno das redes neurais\"\n",
        "\n",
        "#### 2. **Referências Modernas**\n",
        "- **Goodfellow, I., Bengio, Y., & Courville, A. (2016).** \"Deep Learning\". MIT Press.\n",
        "  - **Conteúdo:** Livro texto abrangente sobre deep learning\n",
        "  - **Disponível online:** [deeplearningbook.org](http://www.deeplearningbook.org)\n",
        "\n",
        "- **Nielsen, M. A. (2015).** \"Neural Networks and Deep Learning\".\n",
        "  - **Característica:** Introdução prática com implementações em Python\n",
        "  - **Disponível online:** [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)\n",
        "\n",
        "### Artigos Seminais por Tópico\n",
        "\n",
        "#### Redes Convolucionais (CNNs)\n",
        "- **LeCun, Y., et al. (1998).** \"Gradient-Based Learning Applied to Document Recognition\".\n",
        "  - **Contribuição:** Introduziu a arquitetura LeNet-5 para reconhecimento de dígitos\n",
        "\n",
        "- **Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).** \"ImageNet Classification with Deep Convolutional Neural Networks\".\n",
        "  - **Impacto:** Revolucionou visão computacional com AlexNet\n",
        "\n",
        "- **Simonyan, K., & Zisserman, A. (2014).** \"Very Deep Convolutional Networks for Large-Scale Image Recognition\".\n",
        "  - **Contribuição:** Introduziu arquitetura VGG\n",
        "\n",
        "#### Redes Recorrentes (RNNs/LSTMs)\n",
        "- **Hochreiter, S., & Schmidhuber, J. (1997).** \"Long Short-Term Memory\".\n",
        "  - **Importância:** Introduziu a arquitetura LSTM\n",
        "\n",
        "- **Cho, K., et al. (2014).** \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\".\n",
        "  - **Contribuição:** Introduziu GRU (Gated Recurrent Unit)\n",
        "\n",
        "#### Redes Generativas (GANs)\n",
        "- **Goodfellow, I., et al. (2014).** \"Generative Adversarial Nets\".\n",
        "  - **Impacto:** Introduziu o conceito de GANs\n",
        "\n",
        "#### Attention e Transformers\n",
        "- **Vaswani, A., et al. (2017).** \"Attention Is All You Need\".\n",
        "  - **Revolução:** Introduziu arquitetura Transformer, base para modelos modernos de NLP\n",
        "\n",
        "---\n",
        "\n",
        "##  Referências Técnicas Específicas\n",
        "\n",
        "### Funções de Ativação\n",
        "- **Glorot, X., & Bengio, Y. (2010).** \"Understanding the difficulty of training deep feedforward neural networks\".\n",
        "  - **Contribuição:** Inicialização Xavier/Glorot\n",
        "\n",
        "- **He, K., et al. (2015).** \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\".\n",
        "  - **Importância:** Introduziu inicialização He e ReLU paramétrica\n",
        "\n",
        "### Otimização\n",
        "- **Kingma, D. P., & Ba, J. (2014).** \"Adam: A Method for Stochastic Optimization\".\n",
        "  - **Impacto:** Introduziu o otimizador Adam\n",
        "\n",
        "- **Ruder, S. (2016).** \"An overview of gradient descent optimization algorithms\".\n",
        "  - **Revisão:** Comparação abrangente de algoritmos de otimização\n",
        "\n",
        "### Regularização\n",
        "- **Srivastava, N., et al. (2014).** \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\".\n",
        "  - **Contribuição:** Introduziu técnica de Dropout\n",
        "\n",
        "- **Ioffe, S., & Szegedy, C. (2015).** \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\".\n",
        "  - **Impacto:** Introduziu Batch Normalization\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##  Ferramentas e Frameworks\n",
        "\n",
        "### Principais Frameworks\n",
        "1. **TensorFlow**\n",
        "   - **Documentação:** [tensorflow.org](https://www.tensorflow.org)\n",
        "   - **GitHub:** [github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)\n",
        "\n",
        "2. **PyTorch**\n",
        "   - **Documentação:** [pytorch.org](https://pytorch.org)\n",
        "   - **GitHub:** [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)\n",
        "\n",
        "3. **Keras**\n",
        "   - **Documentação:** [keras.io](https://keras.io)\n",
        "   - **GitHub:** [github.com/keras-team/keras](https://github.com/keras-team/keras)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##  Leitura Recomendada por Nível\n",
        "\n",
        "### Iniciante\n",
        "1. **\"Make Your Own Neural Network\"** - Tariq Rashid\n",
        "2. **\"Deep Learning with Python\"** - François Chollet\n",
        "3. **Blog: \"Machine Learning is Fun!\"** - Adam Geitgey\n",
        "\n",
        "### Intermediário\n",
        "1. **\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"** - Aurélien Géron\n",
        "2. **\"Deep Learning for Computer Vision\"** - Rajalingappaa Shanmugamani\n",
        "3. **Fast.ai cursos práticos**\n",
        "\n",
        "### Avançado\n",
        "1. **\"Deep Learning\"** - Goodfellow, Bengio, Courville\n",
        "2. **\"Pattern Recognition and Machine Learning\"** - Christopher Bishop\n",
        "3. **Artigos originais das arquiteturas fundamentais**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iyGcNiOm1i8O"
      }
    }
  ]
}